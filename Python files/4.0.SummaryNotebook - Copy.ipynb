{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c28e783-17cf-4798-8231-4a6be2111c13",
   "metadata": {},
   "source": [
    "## ENVM 1502 -River Basin Hydrology and Water Management (2022/23 Q3)\n",
    "- [Anne Versleijen](https://github.com/anneversleijen) (4542893)\n",
    "- [David Haasnoot](https://github.com/Daafip)  (4897900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d31ec-47db-4bce-82de-8cab88051b5f",
   "metadata": {},
   "source": [
    "### loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9298ce48-4d51-474b-bb73-3259bcbc8804",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cartopy.crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mccrs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m rioshow\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cartopy.crs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import warnings\n",
    "import datetime\n",
    "import regionmask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import root\n",
    "from scipy.stats import theilslopes\n",
    "\n",
    "# webscraping Q data\n",
    "import json\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# plotting/mapmaking\n",
    "import rasterio\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gpd\n",
    "from rasterio.plot import show as rioshow\n",
    "from ismn.interface import ISMN_Interface\n",
    "from cartopy.io.img_tiles import OSM\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# own geospatial functions\n",
    "from geospatial_functions import get_background_map\n",
    "from geospatial_functions import reproject_raster\n",
    "from geospatial_functions import remove_below_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f66cd5-6054-4478-8f97-dd789a084522",
   "metadata": {},
   "source": [
    "### specifiying directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3937801-d743-4f88-85a7-8861063556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "home_path = os.path.dirname(path)\n",
    "data_folder = f'{home_path}\\\\Data'\n",
    "gis_folder = f'{home_path}\\\\GIS'\n",
    "data_folder_grace = f'{home_path}\\\\Data\\\\Grace'\n",
    "soil_moisture_path  = f'{os.path.dirname(path)}\\\\Data\\\\Soilmoisture'\n",
    "ENVM1502_link_path = os.path.dirname(os.path.dirname(os.path.dirname(path)))\n",
    "ENVM1502_data_path = f'{ENVM1502_link_path}\\\\Unit 2\\\\Ex3 - Soil moisture\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea31ae-1ea4-4faf-be8d-d483f40856f6",
   "metadata": {},
   "source": [
    "### loading needed geospatial layers\n",
    "these are prepared using data from hydrosheds and dem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9011-4709-481b-8a9e-a590d15d9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading other layers\n",
    "outline           = gpd.read_file(f\"{gis_folder}\\\\hudson_basin_26918.gpkg\",driver=\"GPKG\",crs=\"EPSG:26918\")\n",
    "main_rivers       = gpd.read_file(f\"{gis_folder}\\\\main_rivers_hudson_basin.gpkg\",driver=\"GPKG\",crs=\"EPSG:26918\")   \n",
    "all_rivers        = gpd.read_file(f\"{gis_folder}\\\\rivers_hudson_basin.gpkg\",driver=\"GPKG\",crs=\"EPSG:26918\")   \n",
    "gdf_thiessen      = gpd.read_file(f'{gis_folder}\\\\Thiessen_prcp.shp',crs=\"EPSG:26918\")\n",
    "outline_buffered  = gpd.read_file(f\"{gis_folder}\\\\hudson_basin_buffered.gpkg\",crs=\"EPSG:26918\")\n",
    "\n",
    "# fixing crs\n",
    "for layer in [outline,main_rivers,all_rivers, gdf_thiessen,outline_buffered]:\n",
    "    layer.geometry = layer.geometry.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ad13f-7248-4a2b-9fa8-1327d67b4df9",
   "metadata": {},
   "source": [
    "# More indepth analyses & data can be found on [github](https://github.com/Daafip/ENVM1502-Catchment-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008bd97-c79a-48e1-bec0-5de647b80e8f",
   "metadata": {},
   "source": [
    "# 0. Dataset \n",
    "## 0.1.1 & 0.1.2 - P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1c648-a944-478b-a9d2-06e461712036",
   "metadata": {},
   "source": [
    "All data is downloaded from:  https://www.ncei.noaa.gov/cdo-web/. The raw data file is too large to be stored, individual station data can be found [here](https://github.com/Daafip/ENVM1502-Catchment-model/tree/main/Data/P/data%20per%20station) in the repo. \n",
    "\n",
    "Initially the mean over the whole area was taken, however this removes the peaks. To combat this thiessen polygons were made to make a better representation of the data. This can be found in notebook [0.1.2](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/0.1.2.P_splitting_into_theissen.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2ad2-a7ec-44d3-8359-0c1f6036f378",
   "metadata": {},
   "source": [
    "**load in station data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8faeb2-df99-48da-ac07-14a8ea6b63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable to store data\n",
    "df_lst_input = []\n",
    "station_lst = []\n",
    "station_index_dict= {}\n",
    "\n",
    "# read in all precipitation files\n",
    "files = glob.glob(f\"{data_folder}\\\\P\\\\data per station\\\\*_prcp.csv\")\n",
    "for index, file in enumerate(files):\n",
    "    df = pd.read_csv(file, index_col='DATE', usecols=['DATE', 'PRCP', 'STATION', 'LATITUDE', 'LONGITUDE'],  delimiter=',', parse_dates=True)\n",
    "    station_lst.append([df.iloc[0].STATION,df.iloc[0].LATITUDE, df.iloc[0].LONGITUDE])\n",
    "    df_lst_input.append(df)\n",
    "    station_index_dict[f\"{df.iloc[0].STATION}\"] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bdb4eb-e29d-4d11-8e33-9ee5be0ccb3b",
   "metadata": {},
   "source": [
    "**combine raw data into layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f4ce5-4ebd-4d9f-8227-c2745e25acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of the ids\n",
    "station_arr = np.array(station_lst)\n",
    "df_stations = pd.DataFrame(data=list(zip(station_arr[:,0], station_arr[:,1],station_arr[:,2])),\n",
    "                           columns = ['STATION', 'LATITUDE', 'LONGITUDE'],)\n",
    "# combine the points into a geodataframe to be plotted\n",
    "geom  = gpd.points_from_xy(df_stations.LONGITUDE,df_stations.LATITUDE)\n",
    "gdf_stations = gpd.GeoDataFrame(data=df_stations,geometry=geom, crs=\"EPSG:4326\")\n",
    "gdf_stations_in_area = gdf_stations[gdf_stations.within(outline.iloc[0].geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be561437-d6db-4e4c-a9df-f2d4e88e34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the stations & polygons\n",
    "fig, ax = plt.subplots(1)\n",
    "gdf_thiessen.plot(ax=ax)\n",
    "gdf_stations_in_area.plot(ax=ax,color=\"C1\",markersize=3) .plot()\n",
    "outline.plot(ax=ax,edgecolor=\"C6\",facecolor=\"None\")\n",
    "ax.set_title(f\"example of what all {len(gdf_stations_in_area)} stations with thiessen polygons would look like\")\n",
    "ax.set_xlabel(\"Latitude\")\n",
    "ax.set_ylabel(\"Longitude\");\n",
    "\n",
    "# add background\n",
    "bounds_stations = (ax.get_xlim()[0], ax.get_ylim()[0], ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "with rasterio.open(get_background_map(\"P_stations\", bounds_stations)) as r:\n",
    "    rioshow(r, ax=ax,zorder=-10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81546c-70db-4bab-8aba-6d3069f1e957",
   "metadata": {},
   "source": [
    "In reality not all stations have data at once, only 4 stations stand between 1950 and 2020, to combat this, we create thiessen polygons for all combinations of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c21b8-3533-4267-8a85-9d1f148924ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_joined = gdf_stations_in_area.sjoin(gdf_thiessen)\n",
    "lst_dfs_concat = []\n",
    "for i in gdf_joined.index:\n",
    "    station_name = gdf_joined.loc[i,\"STATION\"]\n",
    "    station_area = gdf_joined.loc[i,\"area\"]\n",
    "    df = df_lst_input[station_index_dict[station_name]][[\"PRCP\"]].copy()\n",
    "    df.rename(columns={'PRCP':f'{station_name}'},inplace=True)\n",
    "    lst_dfs_concat.append(df)\n",
    "df_combined = pd.concat(lst_dfs_concat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae955077-9ada-4121-8c08-264c973d5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_year = 50 # threshold for the number of years to be opperating\n",
    "# drop columns depending on the threshold\n",
    "df_selected_combined = df_combined.loc[\"1950\":\"2020\"].dropna(axis=1,thresh=365*n_year)\n",
    "# count the amount of nan values\n",
    "count_nans = df_selected_combined.apply(lambda x: np.isnan([x.loc[col] for col in x.index])).sum(axis=1)\n",
    "# plot the amount of not-nan-values\n",
    "(len(df_selected_combined.columns)- count_nans).plot(lw=0,marker=\".\",ms=1)\n",
    "# beautify\n",
    "plt.title(\"Amount of opperating stations\")\n",
    "plt.ylabel(\"Amount of stations\")\n",
    "plt.xlabel(\"Date\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a04be-6558-4695-a331-fbc5ebdb31de",
   "metadata": {},
   "source": [
    "to speed thing up the threshold was lowered in this case, but a threshold of 20 continous years was used to generate the data set. An example is given below of how the polygon code looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7725a49-e827-4e49-8153-bea4d1e6357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create thiessen polygons\n",
    "from longsgis import voronoiDiagram4plg\n",
    "def draw_thiessen_polygons(stations, boundary):\n",
    "    \"\"\"https://pypi.org/project/voronoi-diagram-for-polygons/\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    vd = voronoiDiagram4plg(stations, boundary)\n",
    "    return vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a00129-754d-43c1-a6d0-408635476ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(df_selected_combined.columns.to_list())\n",
    "series_stations_not_na_txt = df_selected_combined.apply(lambda x: ','.join(columns[~np.isnan([x.loc[col] for col in x.index])]),axis=1)\n",
    "df_stations_not_na_txt = pd.DataFrame(data=series_stations_not_na_txt,columns=[\"names_station\"])\n",
    "total_area = outline.to_crs(\"EPSG:26918\").area.sum()\n",
    "lst_df_joined = []\n",
    "dict_df_joined = {}\n",
    "for index, combination in enumerate(df_stations_not_na_txt['names_station'].unique()[2:3]):\n",
    "    ids_to_thiessen = combination.split(\",\")\n",
    "    df_station_to_thiessen = gdf_stations_in_area.set_index(\"STATION\").loc[ids_to_thiessen]\n",
    "    thiessen_polygons = draw_thiessen_polygons(df_station_to_thiessen, outline)\n",
    "    thiessen_polygons = thiessen_polygons.to_crs(\"epsg:26918\")\n",
    "    df_station_to_thiessen = df_station_to_thiessen.to_crs(\"epsg:26918\")\n",
    "    thiessen_polygons[\"Area_m2\"] = thiessen_polygons.area\n",
    "    df_joined = df_station_to_thiessen.sjoin(thiessen_polygons)\n",
    "    \n",
    "    # save the outputs to be used later, dict to get the index, list to store the data\n",
    "    lst_df_joined.append(df_joined)\n",
    "    dict_df_joined[combination] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710537b-0c3b-46d3-b89c-e8bf28eed2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "df_station_to_thiessen.plot(ax=ax, color=\"C1\",zorder=10,markersize=5)\n",
    "thiessen_polygons.plot(ax=ax)\n",
    "ax.set_title(\"example of first polygon used\")\n",
    "    \n",
    "# add background\n",
    "old_outline_path = get_background_map(\"outline\", bounds_stations)\n",
    "background_epsg26918 = reproject_raster(old_outline_path, \"epsg:26918\",ending=\"tif\")\n",
    "with rasterio.open(background_epsg26918) as r:\n",
    "    rioshow(r, ax=ax,zorder=-10)\n",
    "    \n",
    "ax.set_xlim((4.5e5,7e5))\n",
    "ax.set_ylim((4.64e6,4.9e6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035083e-6d1f-4a56-988e-4b0b54bed521",
   "metadata": {},
   "source": [
    "With the polygons generated the data can then be added by multiplyin P with the fraction & summing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd8e1b-6576-4aac-97ee-55d3c014c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_parquet(f\"{data_folder}\\\\P\\\\weighted_average_P.parquet\")\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "df_p.plot(ax=ax)\n",
    "ax.set_title(\"combined distribution of stations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9834e40-cfca-4db8-888b-a2bc51e0474b",
   "metadata": {},
   "source": [
    "## 0.1.3 - T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084d2ab-37ee-4232-8b5c-a4ae0b717fa8",
   "metadata": {},
   "source": [
    "Similar to P, T was loaded in from station data obtained from https://www.ncei.noaa.gov/cdo-web/. More info in notebook [0.1.3](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/0.1.3.T_combining_selected_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6a6e0-f2d2-4fc1-b478-54d95d79c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations = gpd.read_file(f'{gis_folder}\\\\location_temperature_stations_upperbains_26918.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e6598-20dd-4a0a-9ec2-7c36d10e29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "gdf_stations.plot(ax=ax, color=\"C1\",zorder=10,markersize=5)\n",
    "outline.to_crs(\"epsg:26918\").plot(ax=ax,edgecolor='C3',facecolor=\"none\")\n",
    "ax.set_title(\"Temperature stations\")\n",
    "    \n",
    "# add background\n",
    "old_outline_path = get_background_map(\"outline\", bounds_stations)\n",
    "background_epsg26918 = reproject_raster(old_outline_path, \"epsg:26918\",ending=\"tif\")\n",
    "with rasterio.open(background_epsg26918) as r:\n",
    "    rioshow(r, ax=ax,zorder=-10)\n",
    "    \n",
    "ax.set_xlim((4.5e5,7e5))\n",
    "ax.set_ylim((4.64e6,4.9e6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f023e07-c75b-491c-9fdb-b255ba1ed2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_ids = gdf_stations[[\"station ID\"]].rename(columns={\"station ID\":\"station_id\"})\n",
    "lst_t_dfs_max = []\n",
    "lst_t_dfs_min = []\n",
    "lst_elevation = []\n",
    "for ids in df_station_ids.station_id.values:\n",
    "    df_in = pd.read_parquet(f\"{data_folder}\\\\T\\\\data parquet basin\\\\temp_{ids}.parquet\")\n",
    "    lst_elevation.append([ids,df_in[\"ELEVATION\"].iloc[0]])\n",
    "    df_in[\"DATE\"] = pd.to_datetime( df_in[\"DATE\"])\n",
    "    df_in = df_in.set_index(\"DATE\")\n",
    "    df_in[\"TMAX\"] = df_in[\"TMAX\"].apply(lambda x: x if (x < 100) else np.nan)\n",
    "    df_in[\"TMIN\"] = df_in[\"TMIN\"].apply(lambda x: x if (x < 100) else np.nan)\n",
    "    df_in = df_in.rename(columns={\"TMAX\":f\"TMAX_{ids}\",\"TMIN\":f\"TMIN_{ids}\"})\n",
    "    df_in_max = df_in.drop(columns=['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION',f\"TMIN_{ids}\"])\n",
    "    df_in_min = df_in.drop(columns=['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION',f\"TMAX_{ids}\"])\n",
    "    lst_t_dfs_max.append(df_in_max)\n",
    "    lst_t_dfs_min.append(df_in_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723fc11-5403-44cc-9216-49f5ff604b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_max = pd.concat(lst_t_dfs_max,axis=1)\n",
    "df_combined_mean_max = df_combined_max.mean(axis=1)\n",
    "\n",
    "df_combined_min = pd.concat(lst_t_dfs_min,axis=1)\n",
    "df_combined_mean_min = df_combined_min.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6197bf-fe22-4396-9c56-5b39295df64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mean_series = (df_combined_mean_max  +  df_combined_mean_min) /2\n",
    "df_t = pd.DataFrame(data=temp_mean_series,columns=[\"T_MEAN\"])\n",
    "df_t.loc[\"1980\":].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e1e1d-4af3-40dd-ad57-e56e2ab3279d",
   "metadata": {},
   "source": [
    "# 0.2 Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad996b-4389-4aee-9822-91df8a6aabab",
   "metadata": {},
   "source": [
    "**From the USGS website we can extract hydrological units for the basin**<br>\n",
    "[USGS Watershed Boundary Dataset (WBD) for 2-digit Hydrologic Unit - 02 (published 20230306) GeoPackage](https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/GPKG/WBD_02_HU2_GPKG.zip) using [USGS TNM Download (v2.0)](https://apps.nationalmap.gov/downloader/#/)\n",
    "![iamge](Figures\\wanted_hydrological_areas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dd9f6-b465-4e33-9f45-4128f0672c92",
   "metadata": {},
   "source": [
    "**The ids shown on the map can be loaded in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a282815-95c1-432e-afcd-addf88d3222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_wbdhu = gpd.read_file(f\"{gis_folder}\\\\WBDHU8-hudson.gpkg\",driver=\"GPKG\",crs=\"EPSG:4326\")  \n",
    "huc8_ids = \",\".join(gdf_wbdhu.huc8.values)\n",
    "# However we only take the northern part\n",
    "huc8_ids = \",\".join(gdf_wbdhu.iloc[[0,2,3,4,8]].huc8.values)\n",
    "huc8_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447bacc-efb6-442b-876c-58d4e0703aee",
   "metadata": {},
   "source": [
    "Which we can query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df2e03-4182-42bd-acb6-601bf216c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id_rest_query = f\"https://waterservices.usgs.gov/nwis/dv/?format=rdb&huc={huc8_ids}&parameterCd=00060&siteType=ST&siteStatus=all\"\n",
    "page = urlopen(station_id_rest_query)\n",
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d18cc-b4f4-40a1-95dd-1e168da3c79c",
   "metadata": {},
   "source": [
    "& process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03961d43-0a56-427b-b6f2-ec8e183d39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 15th line contains the number of sites:\n",
    "skip_rows = 15\n",
    "line = html.split(\"\\n\")[skip_rows]\n",
    "print(line)\n",
    "n = int(line[line.find(\"following \")+len(\"following \"):line.find(\" site\")].strip())\n",
    "\n",
    "data = html.split(\"\\n\")[skip_rows+1:skip_rows+1+n]\n",
    "data_ordered = []\n",
    "for line in data:\n",
    "    words = line[line.find(\"USGS\"):].split(\" \")\n",
    "    type_id_name = words[:2] + [\" \".join(words[2:])]\n",
    "    data_ordered.append(type_id_name)\n",
    "df_sites = pd.DataFrame(data=data_ordered,columns=[\"provider\",\"site_no\",\"name\"])\n",
    "df_sites.site_no = df_sites.site_no.astype(int)\n",
    "\n",
    "df_sites.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce7fb3-c432-4881-8b9e-2160b9fbb6fb",
   "metadata": {},
   "source": [
    "for these site numbers we can get the location & plot them, see notebook [0.2.2](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/0.2.2.Q_webscrape%20locations%20.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec45ef6-ce7d-4142-bf67-5266cbc06e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(gdf_station, ax):\n",
    "    legend_field = []\n",
    "\n",
    "    # stations\n",
    "    gdf_station.plot(ax=ax, markersize=2, color=\"C1\")\n",
    "    legend_field.append(mpl.lines.Line2D([],[],color='C1',linewidth=0,marker=\".\",label='station'))\n",
    "\n",
    "    outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "    main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,zorder=-1,lw=1.5)\n",
    "    all_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.2,zorder=-2,lw=0.4)\n",
    "\n",
    "    bounds_stations = (ax.get_xlim()[0], ax.get_ylim()[0], ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "\n",
    "    # add background\n",
    "    with rasterio.open(get_background_map(\"outline\", bounds_stations)) as r:\n",
    "        rioshow(r, ax=ax,zorder=-10)\n",
    "        \n",
    "    ax.set_xlim(-75.8,-72.8)\n",
    "    ax.set_ylim(40.5, 44.5)\n",
    "    # remove lat/lon markers\n",
    "    # ax.set_xticks([])\n",
    "    # ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353096d-2b99-4df9-8971-343b3af8e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_station = gpd.read_file(f\"{gis_folder}\\\\discharge_stations.gpk\", driver=\"GPKG\")\n",
    "# fixing indexing\n",
    "gdf_station.index = gdf_station.apply(lambda x: int(x.site_no[1:]),axis=1)\n",
    "gdf_station       = gdf_station.drop(columns=[\"site_no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44320a-40da-4f50-881d-e52478027d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "plot_map(gdf_station, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73930b6-53f0-425c-a780-24770b488c30",
   "metadata": {},
   "source": [
    "These sites can then be loaded in, which can be found in notebook [0.2.3](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/0.2.3.Q_Loading%20discharge%20data.ipynb).<br>\n",
    "We focussed on station 1358000 as this was at the bottom of the catchement closest to the outflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45f98e-2ffb-43ed-a602-e0a133c012cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_end_date_csv = f\"{data_folder}\\\\Q_ids_with_begin-end-date.csv\"\n",
    "df_id_dates = pd.read_csv(begin_end_date_csv,index_col=0)\n",
    "df_id_dates.set_index(\"ID\",inplace=True)\n",
    "\n",
    "lst_data   = []\n",
    "lst_errors = []\n",
    "found_ids  = []\n",
    "\n",
    "for path in glob.glob(f\"{data_folder}\\\\Q\\\\1*.parquet\"):\n",
    "    df = pd.read_parquet(path)\n",
    "    # check columns integrity, else this will fail\n",
    "    df = df[['agency_cd', 'site_no', 'Data codes', 'Q_m3_s_mean','date']]\n",
    "    df.index = pd.to_datetime(df.date)\n",
    "    df = df[df[\"Q_m3_s_mean\"]>-999]\n",
    "    site_no = int(str(df['site_no'].iloc[0])[1:])\n",
    "    found_ids.append(site_no)\n",
    "    area = df_id_dates.loc[site_no,\"Area_m2\"]\n",
    "    df[\"Area_USGS\"] = area\n",
    "    df[\"Q_mm_d\"]  = df[\"Q_m3_s_mean\"]  * 86400 / area * 1000  # s -> d, m3 -> mm\n",
    "    lst_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b33d7-95dc-4246-9210-9513f444dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_index = list(found_ids).index(1358000)\n",
    "wanted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fa633-3680-453f-a1d3-6baceff3bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(ax, station_id):\n",
    "    legend_field = []\n",
    "\n",
    "    # stations\n",
    "    gdf_station.plot(ax=ax, markersize=2, color=\"C1\")\n",
    "    legend_field.append(mpl.lines.Line2D([],[],color='C1',linewidth=0,marker=\".\",label='station'))\n",
    "\n",
    "    outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "    main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,zorder=-1,lw=1.5)\n",
    "    all_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.2,zorder=-2,lw=0.4)\n",
    "\n",
    "    bounds_stations = (ax.get_xlim()[0], ax.get_ylim()[0], ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "\n",
    "    # add background\n",
    "    with rasterio.open(get_background_map(\"outline\", bounds_stations)) as r:\n",
    "        rioshow(r, ax=ax,zorder=-10)\n",
    "    \n",
    "    # remove lat/lon markers\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # zoom on one station\n",
    "    given_station = gdf_station.loc[[station_id]]\n",
    "    given_station.plot(ax=ax, color=\"C4\",markersize=40)\n",
    "    x, y = given_station.geometry.x.iloc[0], given_station.geometry.y.iloc[0]\n",
    "    n = 1\n",
    "    ax.set_xlim(x- n,x + n)\n",
    "    ax.set_ylim(y -(2*n/3),y + (2*n/3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ab5de-049b-4d84-8d01-d4ec9881daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lst_data[wanted_index]\n",
    "fig, ax = plt.subplots(1,figsize=(15,3))\n",
    "ax2 = ax.inset_axes([.79,.45, 0.55,0.55])\n",
    "plot_map(ax2, int(df[\"site_no\"].iloc[0]))\n",
    "df[['Q_mm_d']].plot(ax=ax, alpha=0.5)\n",
    "ax.set_title(f'Discharge for site: #{df[\"site_no\"].iloc[0]}')\n",
    "ax.legend().remove()\n",
    "ax.set_ylabel(\"Discharge in mm/d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13395a0d-6ea5-4b90-a0ce-d3708d3964c3",
   "metadata": {},
   "source": [
    "after a closer look this gaues seems to be missing some data between 1997 & 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0722063-ecf8-4ad8-884f-096c2b2419c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_data[wanted_index][\"Q_mm_d\"].loc[\"1997-07\":\"2001\"].plot(marker=\".\")\n",
    "plt.title(\"missing data for station 1358000\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd1a69-c14e-4fea-a27f-57d9964b87e6",
   "metadata": {},
   "source": [
    "Instead opt for adding two gauges together: 01357500 & 01335754 \n",
    "\n",
    "![river_split](Figures\\river_split.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fbaca-3c20-4d87-badd-24b55196a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_E_W_path = glob.glob(f\"{data_folder}\\\\Q\\\\combining_1357500_1335754.parquet\")\n",
    "\n",
    "df_combined_E_W = pd.read_parquet(df_combined_E_W_path[0], columns=['Q_m3_s_mean','date'])\n",
    "df_combined_E_W.index = pd.to_datetime(df_combined_E_W.date)\n",
    "df_combined_E_W.rename(columns={'Q_m3_s_mean':'Q'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25aeb5-e37c-4b97-91bf-6e2d84d6f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_combined_E_W[\"Q\"].loc[\"1997-07\":\"2000\"].plot(label=\"New\")\n",
    "lst_data[wanted_index][\"Q_mm_d\"].loc[\"1997-07\":\"2000\"].plot(ax=ax,label=\"old data\")\n",
    "ax.legend()\n",
    "ax.set_title(\"comparing the two gauges\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606b15f-6c77-4d4a-bd11-de4509c3464e",
   "metadata": {},
   "source": [
    "# 0.3 E_pot\n",
    "-> handled in 2.5 & 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15045ea4-dfe0-4df5-b7ab-78f0c5ffeacf",
   "metadata": {},
   "source": [
    "# 0.4 Combining these four variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde807f-b80e-4b36-bbab-75f42d69c077",
   "metadata": {},
   "source": [
    "The gleam data is only availible from the 1980s so this is taken as the begin of the data period, full details in notebook [0.4]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fc5ed-3f88-4052-85bb-76e940533316",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_forcing = pd.read_parquet(f\"{data_folder}\\\\combined_data.parquet\")\n",
    "combined_forcing = combined_forcing.loc[\"1980\":\"2020\"]\n",
    "for i, col in enumerate([\"Pev\",\"Q\",\"P\"]):\n",
    "    combined_forcing[col].plot(lw=2,marker=\".\",zorder=2-i,label=col)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb5339-e294-4258-8f49-94ea4849515d",
   "metadata": {},
   "source": [
    "# 1.1 - Budyko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a9b3d-0309-48b9-b823-ee384c895417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def budyko(df_data_1, show=True):\n",
    "    df_data_1.dropna(inplace=True)\n",
    "    df_data = df_data_1.copy()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #Calculate mean\n",
    "    p_mean = df_data_1.P.mean()\n",
    "    q_mean = df_data_1.Q.mean()\n",
    "    ep_mean = df_data_1.Pev.mean()\n",
    "    \n",
    "    df_data_1['Ea_P'] = 1 - df_data_1['Q'] / df_data_1['P']\n",
    "    \n",
    "    EA_P = 1 - q_mean / p_mean\n",
    "    EP_P  = ep_mean / p_mean\n",
    "    \n",
    "    EP_P_max = int(EP_P + 5)\n",
    "    EP_P_array = np.linspace(1e-9, EP_P_max, EP_P_max*10)\n",
    "    \n",
    "    #Calculate theoretical Budyko\n",
    "    budyko = (EP_P_array * np.tanh(1 / EP_P_array) * (1 - np.exp(-EP_P_array))) ** 0.5\n",
    "    budyko_EP = (EP_P * np.tanh(1 / EP_P) * (1 - np.exp(-EP_P))) ** 0.5\n",
    "    budyko_EA = budyko_EP * p_mean\n",
    "\n",
    "    dS_dt = p_mean - q_mean - budyko_EA\n",
    "    \n",
    "    #Make figure\n",
    "    ax.plot([0,1], [0,1], 'r')\n",
    "    ax.plot([1, EP_P_max], [1, 1], 'b')\n",
    "    ax.set_xlabel('$E_p$/P [-]')\n",
    "    ax.set_ylabel('$E_A$/P [-]')\n",
    "\n",
    "    ax.set_title(f'Long term water balance, dS/dT = {dS_dt:.2f} ')\n",
    "    ax.plot(EP_P, EA_P, 'o', label=f'Partioning point GLEAM data')\n",
    "    ax.plot(EP_P_array, budyko, '--', alpha=0.6, label=f'Budyko (1948) ')\n",
    "    ax.legend()\n",
    "    \n",
    "    if show != True:\n",
    "        plt.close()\n",
    "\n",
    "    return df_data, dS_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a2466-ed3c-433d-9a05-42d8414b2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_term, dS_dt  = budyko(combined_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32313c1e-cf35-4d6c-8512-d52d93d367ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_duration(df, ax, show=True):\n",
    "    \n",
    "    df['Q'].dropna(inplace=True) #Drop the nan values\n",
    "    df_q = df[[\"Q\"]].rename(columns={\"Q\":\"R\"})\n",
    "    \n",
    "    df_q.sort_values(\"R\",ascending=False,inplace=True)\n",
    "    df_q = df_q.reset_index().reset_index()\n",
    "    df_q.set_index(\"index\",inplace=True)\n",
    "    df_q.rename(columns={\"level_0\":\"m\"}, inplace=True)\n",
    "    \n",
    "    n = len(df_q)\n",
    "    df_q['m'] = df_q.apply(lambda x: x.m+1, axis=1)\n",
    "    df_q['p'] = df_q.apply(lambda x: x.m / (n+1), axis=1)\n",
    "    \n",
    "    # fig, ax = plt.subplots(2,1,figsize=(6,8))\n",
    "    # fig.tight_layout(h_pad=6)\n",
    "    \n",
    "    ax[0].set_ylabel(\"Runoff Q in [$mm/d$]\")\n",
    "    ax[0].set_xlabel(\"Date\")\n",
    "    ax[0].set_title('Discharge Hudson River')\n",
    "    df[[\"Q\"]].plot(ax = ax[0], color=\"C0\", xlabel='Date')\n",
    "    \n",
    "    ax[1].set_ylabel(\"Runoff Q in [$mm/d$]\")\n",
    "    ax[1].set_xlabel(\"Excedance probability [-]\")\n",
    "    ax[1].set_title(f\"Run off exceedance probability \")\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].grid()\n",
    "    ax[1].plot(df_q.p, df_q.R, color=\"C0\")\n",
    "    \n",
    "    if show != True:\n",
    "        plt.close()\n",
    "\n",
    "    return df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e5e77-745f-4ed8-bd17-204c0ee1bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correlation(df, ax, show=True):\n",
    "    df.dropna(inplace=True)\n",
    "    lag = range(50)\n",
    "    corr_coeff_1 = []\n",
    "    \n",
    "    #calculate correlation coefficient for different time lags\n",
    "    for i in lag:\n",
    "        corr_coeff_1.append(df['Q'].autocorr(lag=i))\n",
    "    \n",
    "    #plot discharge and results\n",
    "    # fig, ax = plt.subplots(2,1, figsize=(6, 8))\n",
    "    # fig.tight_layout(h_pad=6)\n",
    "    df['Q'].plot(ax=ax[0],color=\"C0\")\n",
    "\n",
    "    ax[0].set_ylabel(\"Runoff Q in [$mm/d$]\")\n",
    "    ax[0].set_xlabel(\"Date\")\n",
    "    ax[0].set_title('Discharge Hudson River')\n",
    "    \n",
    "    \n",
    "    ax[1].plot(lag, corr_coeff_1, color=\"C0\")\n",
    "    \n",
    "    ax[1].set_ylabel(\"Correlation Coefficient $[-]$\")\n",
    "    ax[1].set_xlabel(\"Time lag $[d]$\")\n",
    "    ax[1].set_title(f\"Catchment\")\n",
    "    ax[1].grid()\n",
    "    \n",
    "    if show != True:\n",
    "        plt.close()\n",
    "    \n",
    "    return corr_coeff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526f94d-c3f6-4cb5-8b38-1a95495dac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "fig.tight_layout(h_pad=6, w_pad=6)\n",
    "_ = flow_duration(combined_forcing, [ax[0][0],ax[1][0]])\n",
    "_ = auto_correlation(combined_forcing,[ax[0][1],ax[1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be42fa-f2ed-450d-8c23-ed23d790c6d5",
   "metadata": {},
   "source": [
    "# 1.2 - Extreme value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c737e3d-9325-4080-a04f-01fc2db7821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_df = combined_forcing[[\"P\"]].copy()\n",
    "prcp_df.dropna()\n",
    "prcp_df.rename(columns={\"P\":\"PRCP\"},inplace=True)\n",
    "prcp_df = prcp_df[prcp_df.index >  pd.Timestamp(\"1892-08-01\")] # skip gap in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d3d40-21a5-48e8-9b14-af3081e4143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_maxima(data):\n",
    "    idx_max = data.groupby(data.index.year)['PRCP'].idxmax()\n",
    "    max_list = data.loc[idx_max]\n",
    "    return max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42c798-f401-4f74-8d35-109d67f55307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prcp_maxima = annual_maxima(prcp_df).sort_values(\"PRCP\",ascending=False)\n",
    "df_prcp_maxima.index.name = \"time\"\n",
    "df_prcp_maxima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0b92b-8bbe-45eb-b3a5-6c067178edd1",
   "metadata": {},
   "source": [
    "Calculate emperical statistics for these maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa4d9d-df9c-405e-89bc-751f6ea61b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise, calculate N and expand you annual maxima dataframe with the columns i, p, q, y, T_a\n",
    "M = len(df_prcp_maxima)\n",
    "\n",
    "# indexing tricks to obtain i \n",
    "df_prcp_maxima_ranked = df_prcp_maxima.copy().reset_index().reset_index()\n",
    "df_prcp_maxima_ranked.set_index(\"time\", inplace=True)\n",
    "df_prcp_maxima_ranked['i'] = df_prcp_maxima_ranked.apply(lambda x: int(x['index']) + 1, axis=1)\n",
    "df_prcp_maxima_ranked.drop(columns=['index'],inplace=True)\n",
    "\n",
    "# use i to obtain p\n",
    "df_prcp_maxima_ranked['p'] = df_prcp_maxima_ranked.apply(lambda x: int(x.i) / (M+1), axis=1)\n",
    "# q = 1-p\n",
    "df_prcp_maxima_ranked['q'] = df_prcp_maxima_ranked.apply(lambda x: 1 - x.p, axis=1)\n",
    "\n",
    "# to compute y = -ln(-ln(q))\n",
    "df_prcp_maxima_ranked['y'] = df_prcp_maxima_ranked.apply(lambda x: -np.log(-np.log(x.q)), axis=1)\n",
    "\n",
    "# obtain T_a with 1/p \n",
    "df_prcp_maxima_ranked['T_a'] = df_prcp_maxima_ranked.apply(lambda x: 1 / x.p, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb5150-7310-4420-be15-5ce2ab5728a7",
   "metadata": {},
   "source": [
    "Calculate statistics for gumbel & fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e9f93-d5cb-4b20-9e02-207ad2ae8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise: compute Gumbel parameters (name them sigma and mu)\n",
    "annual_maxima_df_sorted = df_prcp_maxima_ranked.copy()\n",
    "s_R = annual_maxima_df_sorted['PRCP'].std()\n",
    "s_y = annual_maxima_df_sorted['y'].std()\n",
    "y_gem = annual_maxima_df_sorted['y'].mean()\n",
    "R_max_gem = annual_maxima_df_sorted['PRCP'].mean()\n",
    "\n",
    "sigma = s_R / s_y\n",
    "mu = R_max_gem - s_R * y_gem / s_y\n",
    "\n",
    "print(f'scale parameter σ =  {sigma:.3f}')\n",
    "print(f'location parameter µ = {mu:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c670e0-1206-4dbd-bf39-cbb1fed267a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can construct the Gumbel fit and plot the Gumbel line\n",
    "dummy_y = np.arange(-2,6.01,0.2)\n",
    "R_Gumbel = sigma * dummy_y + mu\n",
    "\n",
    "fig, [ax,ax2] = plt.subplots(1,2, figsize=(10,5))\n",
    "fig.tight_layout(w_pad=3)\n",
    "\n",
    "ax.plot(annual_maxima_df_sorted['y'].values, annual_maxima_df_sorted['PRCP'].values, \\\n",
    "        'xk', markersize=7, label='Observed Annual Maxima')\n",
    "ax.plot(dummy_y, R_Gumbel, \\\n",
    "        ':og', label = 'Gumbel Estimate')\n",
    "ax.set_xlabel('Reduced variate y')\n",
    "ax.set_ylabel('Precipitation (mm/day)')\n",
    "ax.legend(loc='best',framealpha=0.5)\n",
    "ax.grid(True)\n",
    "\n",
    "# Here we define the return periods and we inspect the difference between the two return periods\n",
    "T_interest = np.asarray([0.2, 0.5, 1, 1.2, 1.5, 2, 5, 10, 20, 35, 50, 100, 200, 500, 1000])\n",
    "T_a_interest = 1 / (1-np.exp(-1/T_interest))\n",
    "ax2.loglog(T_a_interest,T_interest)\n",
    "ax2.set_xlabel('return period of exceeding an annual maximum $T_a$')\n",
    "ax2.set_ylabel('return period $T$')\n",
    "\n",
    "# real return period for the observed annual maxima\n",
    "# T_a = 1 / p\n",
    "# annual_maxima_df_sorted.loc[:,'T_a'] = T_a\n",
    "# T = -1 / np.log(1 - 1/annual_maxima_df_sorted['T_a'].values)\n",
    "# annual_maxima_df_sorted.loc[:,'T'] = T\n",
    "annual_maxima_df_sorted['T'] = df_prcp_maxima_ranked.apply(lambda x: -1 / np.log(1 - (1/ x[\"T_a\"])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1acf5-a97b-498f-a9d1-5b0be4b96a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_estimate = sigma * (- np.log(1/T_interest)) + mu\n",
    "T = annual_maxima_df_sorted['T'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c989d-4ff5-4b87-83ab-f0e9de5df50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mevpy as mev # only works when this \"Mevpy\" folder is in the folder of this .ipnyb-file (https://github.com/EnricoZorzetto/mevpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a553dc-2a29-4a1f-a4a3-72b467d39c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit GEV parameters (withhout overwriting the sigma and mu from Gumbel)\n",
    "AMS = annual_maxima_df_sorted['PRCP'].values\n",
    "xi, sigma_gev, mu_gev = mev.gev_fit(AMS)\n",
    "\n",
    "F_interest = np.exp(-(1/T_interest))*(1/T_interest)**0\n",
    "\n",
    "# calculate the GEV estimate for our return periods of interest\n",
    "gev_estimate = mev.gev_quant(F_interest, xi, sigma_gev, mu_gev)\n",
    "\n",
    "# convert to dataframe and make a nice table\n",
    "df_gev_estimate = pd.DataFrame(gev_estimate,columns=['GEV estimate (mm/day)'])\n",
    "df_gev_estimate.index = T_interest # Return Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a014188-b876-4e1e-9921-2713492f066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(T, annual_maxima_df_sorted['PRCP'].values, \\\n",
    "        'xk', markersize=7, label='Observed Annual Maxima')\n",
    "# ax.plot(T_interest[:-4], full_series, ':Pb', markersize=7, label = 'Full series')\n",
    "ax.plot(T_interest, gumbel_estimate, ':og', label = 'Gumbel Estimate')\n",
    "ax.plot(T_interest, gev_estimate, ':^r', label='GEV Estimate')\n",
    "ax.set_xlabel('Return period (years)')\n",
    "ax.set_ylabel('Precipitation return level (mm/day)')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best',framealpha=0.5)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a82666-53ec-4929-8b00-46177e7d8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_df[\"YEAR\"] = prcp_df.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95f56a-e2cd-47f9-8b13-d65ab4f3a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit mev parameters for each year\n",
    "N, C, W =  mev.mev_fit(prcp_df, threshold=1)\n",
    "\n",
    "# put this into a nice dataframe\n",
    "df_params_mev = pd.DataFrame([N,C,W]).transpose()\n",
    "df_params_mev.columns = ['N','C','W']\n",
    "\n",
    "# plotting the CDF curves\n",
    "dummy_X = np.arange(0,1000,2)\n",
    "\n",
    "# non-exceedance probability for individual years (here defined as H)\n",
    "H = np.zeros((len(dummy_X),M))\n",
    "zeta = np.zeros((len(dummy_X)))\n",
    "\n",
    "# continue with your own code here:\n",
    "for i in range(len(zeta)):\n",
    "    x = dummy_X[i]\n",
    "    lst = []\n",
    "    for j in range(len(W)):\n",
    "        lst.append((1 - np.exp(-(x/C[j])**W[j]))**N[j])\n",
    "    zeta[i] = 1 / M * np.sum(lst)\n",
    "    \n",
    "# x = dummy_X[0]\n",
    "for j in range(M):\n",
    "    for i in range(len(H)):\n",
    "        x = dummy_X[i]\n",
    "        H[i,j] = (1 - np.exp(-(x/C[j])**W[j]))**N[j]\n",
    "\n",
    "# plot figure like in Enrico's paper\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dummy_X, H)\n",
    "ax.plot(dummy_X, zeta, 'k', linewidth=2, label='MEV')\n",
    "ax.set_xlabel('Daily rainfall totals x [mm]')\n",
    "ax.set_ylabel('Non-exceedance probability $\\zeta(x)$')\n",
    "ax.set_xscale('log')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e3b3-84ae-4b4f-b473-88142f5a7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the MEV solution\n",
    "x0 = np.mean(AMS)\n",
    "mev_estimate, flags =  mev.mev_quant(F_interest, x0, N, C, W)\n",
    "\n",
    "if any(flags) == True:\n",
    "    print('Change starting guess for the numerical solution x0')\n",
    "elif any(flags) == False:\n",
    "    print('No errors have been raised, continue the exercise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc8632-530f-41b7-8f96-0eee667f6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# & including the MEV alternative:\n",
    "# return periods for the CDF curves\n",
    "Ta_c = 1/(1-zeta)\n",
    "T_c  = -1/(np.log(1-(1/Ta_c)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.plot(T, annual_maxima_df_sorted['PRCP'].values, \\\n",
    "        'xk', markersize=7, label='Observed Annual Maxima')\n",
    "ax.plot(T_interest, gumbel_estimate, ':og', label = 'Gumbel Estimate')\n",
    "ax.plot(T_interest, gev_estimate, ':^r', label='GEV Estimate')\n",
    "ax.plot(T_interest, mev_estimate, ':sm', label='MEV Estimate')\n",
    "mask = (T_c<1100) & (T_c > 0)\n",
    "ax.plot(T_c[mask], dummy_X[mask], 'lightblue', label='MEV alternative')\n",
    "ax.set_xlabel('Return period (years)')\n",
    "ax.set_ylabel('Precipitation return level (mm/day)')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best',framealpha=0.5)\n",
    "ax.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b431cf9-a1d2-494a-8d7f-7cc3c7d26878",
   "metadata": {},
   "source": [
    "# 1.3 - Vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1aa17-5343-4d31-861c-e3ffc5c388a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def water_balance(P, EP, Si_max):\n",
    "\n",
    "    Si = np.zeros(len(P))\n",
    "    Pe = np.zeros(len(P))\n",
    "    for i in range(1, len(P)):\n",
    "        Si[i] = Si[i-1] + P[i] \n",
    "\n",
    "\n",
    "        if Si[i] > Si_max:\n",
    "            Pe[i] = Si[i] - Si_max\n",
    "            Si[i] = Si[i] - Pe[i]\n",
    "\n",
    "        Si[i] = Si[i] - EP[i]\n",
    "        if Si[i] < 0:\n",
    "            Si[i] = 0\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['P'] = P\n",
    "    df['EP'] = EP\n",
    "    df['Si'] = Si\n",
    "    df['Pe'] = Pe\n",
    "    return df\n",
    "\n",
    "def vegetation(df_in, show=True):\n",
    "    df = df_in.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    Si_max = 2.5 # mm\n",
    "    df_wb = water_balance(df.P, df.Pev, Si_max)\n",
    "\n",
    "    mean_P = df_wb['P'].mean()\n",
    "    mean_Pe = df_wb['Pe'].mean()\n",
    "    ratio = mean_Pe / mean_P\n",
    "    \n",
    "    #Plot results\n",
    "    fig, ax = plt.subplots(2, figsize=(10,6))\n",
    "    df_wb['P'].plot(ax=ax[1])\n",
    "    df_wb['Pe'].plot(ax=ax[1], ylabel='[mm/d]')\n",
    "\n",
    " \n",
    "    \n",
    "    df['P'].plot(ax=ax[0], xlabel='Date', ylabel='[mm/d]')\n",
    "    df['Pev'].plot(ax=ax[0], xlabel='Date', ylabel='[mm/d]')\n",
    "    df['Q'].plot(ax=ax[0], xlabel='Date', ylabel='[mm/d]')\n",
    "    \n",
    "    ax[0].legend() \n",
    "    ax[0].set_title(f'Precipitation, Potential Evaporation and Discharge Hudson Basin')\n",
    "    \n",
    "    ax[1].set_xlabel('Date')\n",
    "    ax[1].set_title(f'Interception, with a throughfall ratio of {ratio:.2f}')\n",
    "    ax[1].legend()\n",
    "    fig.tight_layout(h_pad=4)\n",
    "    \n",
    "    if show != True:\n",
    "        plt.close()\n",
    "        \n",
    "    return ratio, df_wb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f073480-89db-43e6-886e-141e4a5c0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio, df = vegetation(combined_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf11f0-d106-4935-a7d0-bfcbc87e7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plant_transpiration(df_in, show=True):\n",
    "    df = df_in.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    P_mean = df['P'].mean()\n",
    "    Q_mean = df['Q'].mean()\n",
    "    Et_dash = P_mean - Q_mean\n",
    "    EP_mean = df['Pev'].mean()\n",
    "    \n",
    "    df['Et'] = df['Pev'] / EP_mean * Et_dash\n",
    "    df['Sd'] = np.zeros(len(df['P']))\n",
    "    \n",
    "    cumsum = 0 \n",
    "    \n",
    "    for i in df['Et'].index:\n",
    "        cumsum += (df.loc[i,\"P\"] - df.loc[i,'Et'])\n",
    "        if cumsum > 0:\n",
    "            cumsum = 0\n",
    "        df.loc[i,\"Sd\"] = np.minimum(0, cumsum)\n",
    "    \n",
    "    unique_years = len(df.index.year.unique())\n",
    "    \n",
    "    Sr = min(df['Sd'])\n",
    "    Sr_index = df['Sd'].idxmin()\n",
    "    \n",
    "    EP_P = EP_mean / P_mean\n",
    "    Et_P =  df['Et'].mean() / P_mean\n",
    "    \n",
    "    fig, ax = plt.subplots(2,figsize=(8,8))\n",
    "    fig.tight_layout(h_pad=4)\n",
    "    #ax2 = ax[0].twinx()  \n",
    "    df['P'].plot(ax=ax[0], label='Precipitation')\n",
    "    df['Pev'].plot(ax=ax[0], color='C1', label='Evaporation')\n",
    "    \n",
    "    ax[0].legend()\n",
    "    #ax2.legend()\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('[mm/d]');\n",
    "    \n",
    "    df['Sd'].plot(ax=ax[1], color='C3')\n",
    "    ax[1].plot(Sr_index, Sr, 'ro')\n",
    "    ax[1].text(Sr_index+datetime.timedelta(100), Sr, f'SR,{unique_years}y', fontsize=9)\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel('Time')\n",
    "    ax[1].set_ylabel('Storage deficit [mm]');\n",
    "    \n",
    "    if show == True:\n",
    "        print(f'The vegetation-accessible water storage volume (or root-zone storage capacity) SR,y is {-Sr:.2f} [mm] over the preceding {unique_years} years')\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return df, Sr, Et_P, EP_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45237c6-3df0-408c-850f-1543186c17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, Sr, Et_P, EP_P = plant_transpiration(combined_forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f32cf-87a5-4071-a3f2-000d9f903270",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb047b-fed3-4dd6-ac2a-7c7ae324766c",
   "metadata": {},
   "source": [
    "This only take from 1894, due to missing days and to remove the remove gap between 1988 and 1892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751eae97-0fa0-4a15-8600-77ba8af16ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_zeros(axes):\n",
    "    ylims_current = {}   #  Current ylims\n",
    "    ylims_mod     = {}   #  Modified ylims\n",
    "    deltas        = {}   #  ymax - ymin for ylims_current\n",
    "    ratios        = {}   #  ratio of the zero point within deltas\n",
    "    for ax in axes:\n",
    "        ylims_current[ax] = list(ax.get_ylim())\n",
    "                        # Need to convert a tuple to a list to manipulate elements.\n",
    "        deltas[ax]        = ylims_current[ax][1] - ylims_current[ax][0]\n",
    "        ratios[ax]        = -ylims_current[ax][0]/deltas[ax]\n",
    "        \n",
    "    for ax in axes:      # Loop through all axes to ensure each ax fits in others.\n",
    "        ylims_mod[ax]     = [np.nan,np.nan]   # Construct a blank list\n",
    "        ylims_mod[ax][1]  = max(deltas[ax] * (1-np.array(list(ratios.values()))))\n",
    "                        # Choose the max value among (delta for ax)*(1-ratios),\n",
    "                        # and apply it to ymax for ax\n",
    "        ylims_mod[ax][0]  = min(-deltas[ax] * np.array(list(ratios.values())))\n",
    "                        # Do the same for ymin\n",
    "        ax.set_ylim(tuple(ylims_mod[ax]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d92179-af95-4ed4-9226-fb8e60d6a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_snow_model = combined_forcing.loc[pd.Timestamp(\"1894-01-01\"):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f66d0-4eb3-49b4-9065-0c42fe2823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T_P = df_all_data_snow_model[[\"T\",\"P\"]].dropna(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,5),sharex=True)\n",
    "df_T_P[['P']].plot(ax=ax,alpha=1)\n",
    "ax.set_ylabel(\"Precipitation in mm\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "df_T_P[[\"T\"]].plot(ax=ax2,color=\"C3\",alpha=0.5,zorder=-10)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.set_ylabel(\"Temperature in $^{\\circ}C$\")\n",
    "\n",
    "align_zeros([ax,ax2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ebc34-67c0-470e-b15a-4c7bf685cce4",
   "metadata": {},
   "source": [
    "**Consider elevation data as this correlates with snow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3839de-821f-48e6-8e1e-6a2f4c6b33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "path = f'{gis_folder}\\\\basin_dem_SRTM.tif'\n",
    "new_path = reproject_raster(path, \"EPSG:4326\")\n",
    "with rasterio.open(new_path, driver='GTiff') as r:\n",
    "        rioshow(r, ax=ax)\n",
    "ax.set_title(\"DEM of the catchment\")\n",
    "ax.set_xlabel(\"Latitude\")\n",
    "ax.set_ylabel(\"Longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7414b1-588d-4de4-a5aa-f6eae906a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\n",
    "    new_path,\n",
    "    driver='GTiff',\n",
    "    count=1,\n",
    "    crs='EPSG:4326' \n",
    "                    ) as r:\n",
    "    dem_data = r.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9983c68-5da1-463e-a572-d8e4b86bd974",
   "metadata": {},
   "source": [
    "From the station data where the temperature is measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4389c-0478-4a7d-8d41-01176f68a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lst_elevation = np.loadtxt(f'{data_folder}\\\\T\\\\elevation_stations.txt',delimiter=\";\",dtype=str)\n",
    "lst_elevation = load_lst_elevation[:,1].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df38e8-adc9-4764-875b-d65049f88927",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax,ax2] = plt.subplots(1,2,sharex=True,sharey=True,figsize=(5,2))\n",
    "fig.tight_layout(w_pad=4)\n",
    "\n",
    "dem_data_in_basin = dem_data[dem_data>-999]\n",
    "_, __, ___ = ax.hist(np.sort(dem_data_in_basin),density=True);\n",
    "ax.axvline(np.mean(dem_data_in_basin),color=\"r\")\n",
    "ax.set_title(\"Histogram of DEM values\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "\n",
    "_, __, ___ = ax2.hist(lst_elevation,density=True)\n",
    "ax2.axvline(np.mean(lst_elevation),color=\"r\",label=\"mean\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"elevation of measurement stations\");\n",
    "ax2.set_ylabel(\"Probability density\")\n",
    "ax2.set_xlabel(\"Elevation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a967d-b492-42f8-b082-7741e5fe9f92",
   "metadata": {},
   "source": [
    "The two histograms above show the distribution of the different elevations. In red the mean is shown. The dem and station measurements are fairly similar, though the stations measure a bit lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826482c-9950-4134-a78c-7c15d1749c93",
   "metadata": {},
   "source": [
    "**Start by taking the mean heights and the mean temperature as one flat plane**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd85b3-0512-4f20-9cf6-f8210582fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_date(date, days=1):    \n",
    "    subtracted_date = pd.to_datetime(date) - datetime.timedelta(days=days)\n",
    "    subtracted_date = subtracted_date.strftime(\"%Y-%m-%d\")\n",
    "    return subtracted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d3258-caea-46fb-bb3a-1136c97a507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_T_P.copy()\n",
    "\n",
    "Er = lst_elevation.mean() #m \n",
    "Tt = -0.5 # degc\n",
    "FM = 3    # mm/deg/d\n",
    "S_0 = 0 # mm - no snow at start\n",
    "\n",
    "# for every day:\n",
    "dt = 1\n",
    "for i, date in enumerate(df.index):\n",
    "    P  = df.loc[date,\"P\"]\n",
    "    TR = df.loc[date,\"T\"]\n",
    "    if i == 0:\n",
    "        df.loc[prev_date(date),\"Ss\"] = S_0 # initial amount snow (0) for non exisiting day\n",
    "        \n",
    "    # if below threshold temperature, assume not first day for similicity\n",
    "    if TR < Tt and i != 0:\n",
    "        df.loc[date,\"Pr\"] = 0                                                     # no rainfall\n",
    "        df.loc[date,\"Ps\"] = P                                                     # precipitation is snow \n",
    "        df.loc[date,\"M\"]  = 0                                                     # Too cold for melt\n",
    "        df.loc[date,\"Ss\"] = df.loc[prev_date(date),\"Ss\"] + df.loc[date,\"Ps\"] * dt # add snow to the prev day\n",
    "        df.loc[date,\"Pl\"] = 0                                                     # no outflow as too cold\n",
    "    # then must be above\n",
    "    else:\n",
    "        df.loc[date,\"Pr\"] = P                                                     # all precip is rainfall\n",
    "        df.loc[date,\"Ps\"] = 0                                                     # no snow \n",
    "        df.loc[date,\"M\"]  = min(df.loc[prev_date(date),\"Ss\"]/dt, FM * (TR - Tt))  # calculate melt\n",
    "        df.loc[date,\"Ss\"] = df.loc[prev_date(date),\"Ss\"] - df.loc[date,\"M\"] * dt  # remove melted snow\n",
    "        df.loc[date,\"Pl\"] = df.loc[date,\"Pr\"] + df.loc[date,\"M\"]                  # outflow = rain + melt\n",
    "        if i == 0:\n",
    "            df.drop(index=prev_date(date),inplace=True) #  remove initial amount snow for non exisiting day\n",
    "        \n",
    "df_case1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bcc86-8ee0-407b-a823-b398d214cf23",
   "metadata": {},
   "source": [
    "**plot for 1997-1999**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44925f11-0654-4ad5-a427-e67929fbdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax,ax3] = plt.subplots(2,1,figsize=(12,5),sharex=True)\n",
    "df_plot = df_case1.loc[\"1997-09-01\":\"1999-09-01\"]\n",
    "df_plot[['Ss',\"Ps\"]].plot(ax=ax)\n",
    "ax.set_ylabel(\"Storage in mm\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "df_plot[[\"T\"]].plot(ax=ax2,color=\"C3\",alpha=0.5,zorder=-10)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.set_ylabel(\"Temperature in $^{\\circ}C$\")\n",
    "\n",
    "align_zeros([ax,ax2])\n",
    "\n",
    "df_plot[['Pl']].plot(ax=ax3,color=\"C2\")\n",
    "ax3.set_ylabel(\"outflow due to in mm\")\n",
    "ax3.set_title(f\"Outflow of snowmodel on one flat plane at {Er:.2f}m\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab9bf-c5f8-484e-9a87-f79eb97351fe",
   "metadata": {},
   "source": [
    " **repeat but with different elevation bands**\n",
    " As we can see bellow, there is a spread of elevation from 0 to around 1000. these can be split into 4 bands:\n",
    "1. `elevation < 250`\n",
    "1. `250 < elevation < 500`\n",
    "1. `500 < elevation < 750`\n",
    "1. `750 < elevation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ce39c-a169-443a-8099-7de820faf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,sharex=True,sharey=True,figsize=(3,3))\n",
    "\n",
    "dem_data_in_basin = dem_data[dem_data>-999]\n",
    "_, __, ___ = ax.hist(np.sort(dem_data_in_basin),density=True);\n",
    "ax.axvline(np.mean(dem_data_in_basin),color=\"r\")\n",
    "ax.set_title(\"Histogram of DEM values\")\n",
    "ax.set_ylabel(\"Probability density\")\n",
    "ax.annotate(f\"$\\mu$ ={np.mean(dem_data_in_basin):.1f}m\",(np.mean(dem_data_in_basin)+12,0.002),color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6204e-b85e-4791-b926-e5a561540769",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = len(dem_data_in_basin[dem_data_in_basin<=250])/ len(dem_data_in_basin)\n",
    "A2 = len(dem_data_in_basin[(dem_data_in_basin>250) & (dem_data_in_basin <=500)])/ len(dem_data_in_basin)\n",
    "A3 = len(dem_data_in_basin[(dem_data_in_basin>500) & (dem_data_in_basin <=750)])/ len(dem_data_in_basin)\n",
    "A4 = len(dem_data_in_basin[(dem_data_in_basin>750)])/ len(dem_data_in_basin)\n",
    "print(f'A1:\\t{A1:.2f}\\nA2:\\t{A2:.2f}\\nA3:\\t{A3:.2f}\\nA4:\\t{A4:.2f}\\nΣ:\\t{A1 + A2 + A3 + A4:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac84169-66ba-483c-be3c-024b269b877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every day:\n",
    "dt = 1\n",
    "\n",
    "Er = 331  # m\n",
    "\n",
    "heights = [125, 375, 625, 875] # m\n",
    "A_lst = [A1, A2, A3, A4]\n",
    "A = sum(A_lst)\n",
    "\n",
    "lst_df         = []\n",
    "lst_df_storage = []\n",
    "for A_index, Ei in enumerate(heights): \n",
    "    df = df_T_P.copy()\n",
    "    # compute for each day\n",
    "    for i, date in enumerate(df.index):\n",
    "        P  = df.loc[date,\"P\"]\n",
    "        TR = df.loc[date,\"T\"]\n",
    "        Ti = TR - 0.6 * (Ei-Er) / 100\n",
    "        \n",
    "        if i == 0:\n",
    "            df.loc[prev_date(date),\"Ss\"] = S_0 # initial amount snow (0) for non exisiting day\n",
    "\n",
    "        # if below threshold temperature, assume not first day for similicity\n",
    "        if Ti < Tt and i != 0:\n",
    "            df.loc[date,\"Pr\"] = 0                                                     # no rainfall\n",
    "            df.loc[date,\"Ps\"] = P                                                     # precipitation is snow \n",
    "            df.loc[date,\"M\"]  = 0                                                     # Too cold for melt\n",
    "            df.loc[date,\"Ss\"] = df.loc[prev_date(date),\"Ss\"] + df.loc[date,\"Ps\"] * dt # add snow to the prev day\n",
    "            df.loc[date,\"Pl\"] = 0                                                     # no outflow as too cold\n",
    "        # then must be above\n",
    "        else:\n",
    "            df.loc[date,\"Pr\"] = P                                                     # all precip is rainfall\n",
    "            df.loc[date,\"Ps\"] = 0                                                     # no snow \n",
    "            df.loc[date,\"M\"]  = min(df.loc[prev_date(date),\"Ss\"]/dt, FM * (Ti - Tt))  # calculate melt\n",
    "            df.loc[date,\"Ss\"] = df.loc[prev_date(date),\"Ss\"] - df.loc[date,\"M\"] * dt  # remove melted snow\n",
    "            df.loc[date,\"Pl\"] = df.loc[date,\"Pr\"] + df.loc[date,\"M\"]                  # outflow = rain + melt\n",
    "            if i == 0:\n",
    "                df.drop(index=prev_date(date),inplace=True) #  remove initial amount snow for non exisiting day\n",
    "    # for each height aggregate:\n",
    "    fi = A_lst[A_index]/A\n",
    "    df[\"Pl\"] = df.apply(lambda x : x.Pl * fi, axis=1)\n",
    "    df_out = df[[\"Pl\"]].rename(columns={\"Pl\":f\"Pl_{A_index+1}\"})\n",
    "    lst_df.append(df_out)\n",
    "    \n",
    "    df[\"Ss_scaled\"] = df.apply(lambda x : x.Ss * fi, axis=1)\n",
    "    df_storage_out = df[[\"Ss\",\"Ss_scaled\"]].rename(columns={\"Ss\":f\"Ss_{A_index+1}\",\"Ss_scaled\":f\"Ss_scaled_{A_index+1}\"})\n",
    "    lst_df_storage.append(df_storage_out)\n",
    "\n",
    "df_case2 = pd.concat(lst_df + lst_df_storage, axis=1)\n",
    "df_case2[\"Pl_tot\"] = df_case2.apply(lambda x : x.Pl_1 + x.Pl_2 + x.Pl_3 + x.Pl_4, axis=1)\n",
    "df_case2[\"Ss_tot\"] = df_case2.apply(lambda x : x.Ss_scaled_1 + x.Ss_scaled_2 + x.Ss_scaled_3 + x.Ss_scaled_4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d4bb1-2785-4232-ac75-62913f9dd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,sharey=True,figsize=(10,3))\n",
    "fig.suptitle(\"Snow melt in four segments\",y=1.04)\n",
    "\n",
    "df_plot = df_case2.loc[\"1997-09-01\":\"1999-09-01\"]\n",
    "df_plot[['Pl_tot']].plot(ax=ax[0])\n",
    "ax[0].set_ylabel(\"Outflow due to snow in mm\")\n",
    "ax[0].set_title(\"Summed outflow\")\n",
    "df_plot[['Pl_1','Pl_2','Pl_3','Pl_4']].plot(ax=ax[1])\n",
    "ax[1].legend(heights,bbox_to_anchor=(1,1),title=\"Contribution \\nheight(m)\")\n",
    "ax[1].set_title(\"individual contributions\");\n",
    "\n",
    "fig, ax = plt.subplots(1,2,sharey=True,figsize=(10,3))\n",
    "fig.suptitle(\"Snow storage in four segments\",y=1.04)\n",
    "\n",
    "df_plot = df_case2.loc[\"1997-09-01\":\"1999-09-01\"]\n",
    "df_plot[['Ss_tot']].plot(ax=ax[0])\n",
    "ax[0].set_ylabel(\"Outflow due to snow in mm\")\n",
    "ax[0].set_title(\"Summed storage\")\n",
    "df_plot[['Ss_scaled_1','Ss_scaled_2','Ss_scaled_3','Ss_scaled_4']].plot(ax=ax[1])\n",
    "ax[1].legend(heights,bbox_to_anchor=(1,1),title=\"Contribution \\nheight(m)\")\n",
    "ax[1].set_title(\"scaled individual contributions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ace33-a751-4097-b2fe-ddc21983ff7b",
   "metadata": {},
   "source": [
    "You nicely see the storages increase and the difference per elevation band can also be seen nicely. The january melt in the 375m band shows up strongly in the the outflow peak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597a041-2e36-4dfe-b915-de9e3bdddbb3",
   "metadata": {},
   "source": [
    "# 1.5 - Flood routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81fa74-5c85-4e24-b962-3176786e4862",
   "metadata": {},
   "source": [
    "Can be found in notebook [1.5](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/1.5.Flood%20routing.ipynb)\n",
    "takes long to run due to large data set. An example of the fit is given below:\n",
    "\n",
    "![Muskingum](Figures/moddeld_vs_measure_muskin_routing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a31695-a99c-499f-9297-013246bb0c23",
   "metadata": {},
   "source": [
    "# 1.6 - Moisture recycling\n",
    "Can be found in notebook [1.6](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/1.6.Moisture_cycle.ipynb), no large insights, most of the mositure in the eastern united states comes from the ocean, likely also due to the gulfstream which passes by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9859f-54f4-459a-8e8e-9fe1c52ec30e",
   "metadata": {},
   "source": [
    "# 2.1 - Surface water classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339ca03-2ed6-468d-a11a-15edf6627290",
   "metadata": {},
   "source": [
    "Landsat data can be downlaoded from https://earthexplorer.usgs.gov/, using landsat 8 level 2 in this case. \n",
    "\n",
    "This data is unprocessed datset very large and thus is not included, the results will be included.\n",
    "\n",
    "Analysis is done in QGIS with the [\n",
    "Semi-Automatic Classification Plugin](https://github.com/semiautomaticgit/SemiAutomaticClassificationPlugin)\n",
    "\n",
    "The data from landsat can be imported, merged into one multiband layer and clipped to the basin area. The bands are remaped as shown in the legend to make water more visible (black). _Note: not all areas area covered, but optted for best fit_\n",
    "\n",
    "This result is left below:     \n",
    "![Figures\\map_satelite_imaging.jpeg](Figures\\map_satelite_imaging.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2c262-15c4-43c0-b9a5-2a37d385a6a9",
   "metadata": {},
   "source": [
    "The result of the plugin calssification using 10 examples of water and 10 of not water results in the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab42725-d1d1-49e2-87a6-86695c738c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.cm.get_cmap(\"winter\")\n",
    "# loading the data in in 28918 crs\n",
    "outline           = gpd.read_file(f\"{gis_folder}\\\\hudson_basin_26918.gpkg\",driver=\"SHP\",crs=\"EPSG:26918\")\n",
    "main_rivers       = gpd.read_file(f\"{gis_folder}\\\\main_rivers_hudson_basin.gpkg\",driver=\"GPKG\",crs=\"EPSG:26918\")   \n",
    "all_rivers        = gpd.read_file(f\"{gis_folder}\\\\rivers_hudson_basin.gpkg\",driver=\"GPKG\",crs=\"EPSG:26918\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0a709-36a9-4b61-b173-b382f466a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_nans_path = remove_below_0(f'{gis_folder}\\\\classification_of_waterbodies_26918.tif',ending=\"tif\")\n",
    "fig, ax = plt.subplots(1)\n",
    "with rasterio.open(fixed_nans_path, driver='GTiff') as r:\n",
    "        img = rioshow(r, ax=ax,cmap=cmap,zorder=100,alpha=0.8)\n",
    "ax.set_title(\"Classification of surface water in northern par of the catchment\")\n",
    "ax.set_xlabel(\"Latitude $(epsg:26918, unit m)$\")\n",
    "ax.set_ylabel(\"Longitude $(epsg:26918)$\")\n",
    "\n",
    "# add other features\n",
    "outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,zorder=-1,lw=1.5)\n",
    "all_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.2,zorder=-2,lw=0.4)\n",
    "\n",
    "# add background\n",
    "with rasterio.open(background_epsg26918) as r:\n",
    "    rioshow(r, ax=ax,zorder=-100)\n",
    "\n",
    "# fix color bar\n",
    "im = img.get_images()[0]\n",
    "fig.colorbar(im,ax=ax,cmap=cmap);\n",
    "\n",
    "ax.set_xlim((4.5e5,7e5))\n",
    "ax.set_ylim((4.64e6,4.9e6));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2e29a-da92-4c53-9707-4feaf4b6a8d6",
   "metadata": {},
   "source": [
    "as some overlapt with rivers and some very small areas so these can be removed by only taking those larger than 10ha, the overlap remaining is merely the reservoirs built on the river rather than the actual course of the river. \n",
    "This can be further filtered to yield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031932d1-826a-499b-92ea-e7142df60cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_water_more10ha = gpd.read_file(f\"{gis_folder}\\\\waterbodies_polygon_above10ha_excluding river.gpkg\")\n",
    "fig, ax  = plt.subplots(1)\n",
    "df_all_water_more10ha.plot(facecolor=\"C0\",ax=ax,edgecolor=\"w\")\n",
    "\n",
    "# add other features\n",
    "outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,zorder=-1,lw=1.5)\n",
    "all_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.2,zorder=-2,lw=0.4)\n",
    "\n",
    "# add background\n",
    "with rasterio.open(background_epsg26918) as r:\n",
    "    rioshow(r, ax=ax,zorder=-100)\n",
    "    \n",
    "ax.set_title(f\"Surface water larger than 10ha \\n with total {df_all_water_more10ha.area.sum()/10**6:.0f}km^2 from landsat classification\")\n",
    "\n",
    "ax.set_xlabel(\"Latitude $(epsg26918, m)$\")\n",
    "ax.set_ylabel(\"Longitude $(epsg26918)$\")    \n",
    "    \n",
    "ax.set_xlim((4.7e5,7e5))\n",
    "ax.set_ylim((4.70e6,4.9e6));\n",
    "\n",
    "# fig.savefig(r\"Figures/surfacewater_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e2a9b-f49e-46f4-8692-6d35b6d46c87",
   "metadata": {},
   "source": [
    "# 2.2 - Remote Precipitation measurement\n",
    "Not done due to time constraints, complexity in finding data & plenty of other availible prepitation data sources used to fullfill learning goals, see 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7cc0c-33db-4fcf-8498-36f97b246545",
   "metadata": {},
   "source": [
    "# 2.3 - Soil moisture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0a701-506f-4770-9fd0-6def714f683a",
   "metadata": {},
   "source": [
    "### In situ\n",
    "Inital parts for in situ soil measurement, downloaded from https://ismn.earth/en/data/, site [Lye Brook 2042](https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788edae-d045-42aa-8b46-c1190b232076",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = OSM()\n",
    "\n",
    "ismn = ISMN_Interface(soil_moisture_path)\n",
    "stems = None\n",
    "\n",
    "depth = (0.05, 2)\n",
    "def read_ismn(depth):\n",
    "    min_depth, max_depth = depth\n",
    "\n",
    "    fig = plt.figure(figsize=(15,4), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(1, 3)\n",
    "    map_ax, ts_ax = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree()), fig.add_subplot(gs[0, 1:])\n",
    "    \n",
    "    # Plot a small overview map with station locations over Open Street Map\n",
    "    ismn.plot_station_locations(variable='soil_moisture', min_depth=min_depth, max_depth=max_depth, markersize=5, text_scalefactor=2, stats_text=True, ax=map_ax)\n",
    "    map_ax.set_extent([-77,-70,40,47])\n",
    "    map_ax.add_image(request, 6)\n",
    "    outline.plot(ax=map_ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "    main_rivers.plot(ax=map_ax, color=\"lightskyblue\",alpha=0.6,zorder=-1,lw=1.5)\n",
    "    all_rivers.plot(ax=map_ax, color=\"lightskyblue\",alpha=0.2,zorder=-2,lw=0.4)\n",
    "    \n",
    "\n",
    "     # Extract the sensors in the chosen depths at the locations and store their data\n",
    "    sensors = []\n",
    "    for network, station, sensor in ismn.collection.iter_sensors(variable='soil_moisture', depth=(min_depth, max_depth)):\n",
    "        data = sensor.read_data()\n",
    "        data.loc[data['soil_moisture_flag'] != 'G', 'soil_moisture'] = np.nan\n",
    "        data = data['soil_moisture']\n",
    "        if sensor.depth[0] == sensor.depth[1]:\n",
    "            depth = f\"{sensor.depth[0]}\"\n",
    "        else:\n",
    "            depth = f\"{sensor.depth[0]}-{sensor.depth[1]}\"\n",
    "        data.name = f\"{station.name} ({depth} m)\"\n",
    "        sensors.append(data)\n",
    "    \n",
    "    global stems\n",
    "    stems = pd.concat(sensors, axis=1)\n",
    "    \n",
    "    stems_plot = stems.loc['2017-01-01':,:]\n",
    "    \n",
    "    p = stems_plot.plot(ax=ts_ax, xlabel='time', ylabel='SM [m³/m³]', title=f'Insitu data at STEMS network ({min_depth} to {max_depth} [m] depth)')\n",
    "    p.legend().remove()\n",
    "\n",
    "read_ismn(depth);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14d692-6bfd-4e71-9b95-0121ee580e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_val_cols = stems.columns.unique()\n",
    "n = len(unique_val_cols)\n",
    "fig, ax = plt.subplots(int((n+1)/3),3,figsize=(15,10),sharex=True,sharey=True)\n",
    "fig.tight_layout(w_pad=5, h_pad=5)\n",
    "lst_dfs = []\n",
    "ax = ax.flatten()\n",
    "for index, col in enumerate(unique_val_cols):\n",
    "    df_depth = stems[col]\n",
    "    lst_dfs.append(pd.DataFrame(data=df_depth.mean(axis=1).resample(\"d\").mean(),columns=[col]))\n",
    "    df_depth.plot(ax=ax[index])\n",
    "    ax[index].legend().remove()\n",
    "    ax[index].set_title(col)\n",
    "stems_merged = pd.concat(lst_dfs,axis=1)\n",
    "stems_merged.plot(ax=ax[-1])\n",
    "ax[-1].legend(bbox_to_anchor=(1,1))\n",
    "ax[-1].set_title(\"Combined\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed44f4-8dde-4c72-b18c-0791e219f0a9",
   "metadata": {},
   "source": [
    "The plot above shows the uncertainty in the data as different sensors report differently over time. Daily averages can be obtained by taking the mean of the observations and then resampelinig on a daily basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e84b9-fed5-4088-b261-0d3908a21194",
   "metadata": {},
   "source": [
    "### Microwave remote sensing -  Soil Moisture Active Passive Mission (SMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18be65-b5c4-4f72-81ec-99ed24022424",
   "metadata": {},
   "outputs": [],
   "source": [
    "year,month,day  = 2021, 7, 3\n",
    "date = f'{year}-{month:02}-{day:02}'\n",
    "\n",
    "y = 40.48908101650877\n",
    "x = -74.0305856727883\n",
    "n_x = 5\n",
    "n_y = n_x\n",
    "\n",
    "smap_raw = xr.open_dataset(f'{ENVM1502_data_path}/LTC_DATA/SMAP_raw_202107.nc').sel(lon=slice(x - n_x, x + n_x),\n",
    "                lat=slice(y + n_y, y - n_y))\n",
    "smap_lprm = xr.open_dataset(f'{ENVM1502_data_path}/LTC_DATA/SMAP_LPRM_202107.nc').sel(lon=slice(x - n_x, x + n_x),\n",
    "                lat=slice(y - n_y, y + n_y))\n",
    "\n",
    "def plot_lprm(day):\n",
    "    \"\"\"\n",
    "    Select and visualise SMAP data at a given time\n",
    "    \"\"\"\n",
    "    date = f'2021-07-{day:02}'\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,3), subplot_kw={'projection': ccrs.Robinson()})\n",
    "    \n",
    "    fig.suptitle(\"SMAP data from passive remote sensing\", fontsize=18, y=1.08)\n",
    "    p1 = smap_raw['tbh'].sel(time=date).plot(transform=ccrs.PlateCarree(), ax=axs[0],  cmap=plt.get_cmap('plasma'), vmin=200, vmax=320, cbar_kwargs={'label': '[°K]'})\n",
    "    axs[0].set_title(f\"{date} \" \"$T_{B,H}$\")\n",
    "    p2 = smap_raw['tbv'].sel(time=date).plot(transform=ccrs.PlateCarree(), ax=axs[1],  cmap=plt.get_cmap('plasma'), vmin=200, vmax=320, cbar_kwargs={'label': '[°K]'})\n",
    "    axs[1].set_title(f\"{date} \" \"$T_{B,V}$\")\n",
    "                \n",
    "    for p in [p1, p2]:\n",
    "        p.axes.coastlines()\n",
    "        \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18,3), subplot_kw={'projection': ccrs.Robinson()})\n",
    "\n",
    "    \n",
    "    p3 = smap_lprm['temperature'].sel(time=date).plot(transform=ccrs.PlateCarree(), ax=axs[0], cmap=plt.get_cmap('hot_r'), vmin=274, vmax=320, cbar_kwargs={'label': '[°K]'})\n",
    "    axs[0].set_title(f\"{date} Surface Temperature\")\n",
    "    \n",
    "    p4 = smap_lprm['vod'].sel(time=date).plot(transform=ccrs.PlateCarree(), ax=axs[1], cmap=plt.get_cmap('Greens'), vmin=0, vmax=0.5, cbar_kwargs={'label': '[-]'})\n",
    "    axs[1].set_title(f\"{date} Vegetation Optical Depth\")\n",
    "    \n",
    "    p5 = smap_lprm['soil_moisture'].sel(time=date).plot(transform=ccrs.PlateCarree(), ax=axs[2], cmap=plt.get_cmap('Blues'), vmin=0, vmax=0.5, cbar_kwargs={'label': '[m³/m³]'})\n",
    "    axs[2].set_title(f\"{date} Soil Moisture\")\n",
    "            \n",
    "    for p in [p3, p4, p5]:\n",
    "        p.axes.add_feature(cartopy.feature.LAND, zorder=0, facecolor='gray')\n",
    "        p.axes.coastlines()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_lprm(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246cd6b0-63c3-4c44-b1be-a0f6b5341a05",
   "metadata": {},
   "source": [
    "\n",
    "### Combining passive & active -Copernicus Climate Data Service CCDS\n",
    "#### Load esa data\n",
    "\n",
    "_This takes quite long_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a7b0f-9926-44c6-a3b3-6782bc55a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist=glob.glob(f'{ENVM1502_data_path}/CDS_DATA/cds_data_esa/C3S*.nc')\n",
    "\n",
    "dt=[datetime.datetime.strptime(x.split('/')[-1].split('-')[-3], '%Y%m%d%H%M%S') for x in filelist]\n",
    "df = pd.DataFrame({'filepath':filelist}, index=dt).sort_index()\n",
    "stack = xr.open_mfdataset(df.filepath.values, engine='netcdf4',combine='nested', concat_dim='time')\n",
    "dslon = stack['lon'].compute()\n",
    "dslat = stack['lat'].compute()\n",
    "dssm = stack['sm'].compute()\n",
    "# dssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92e440-513d-467d-a451-9ebd3b5f8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "year  = 2018\n",
    "month = 6\n",
    "day   = 3\n",
    "date = f'{year}-{month:02}-{day:02}'\n",
    "\n",
    "y = 40.48908101650877\n",
    "x = -74.0305856727883\n",
    "n_x = 5\n",
    "n_y = n_x\n",
    "ny = dssm.sel(lon=slice(x - n_x, x + n_x),\n",
    "                lat=slice(y + n_y, y - n_y),time=date)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(15,3), subplot_kw={'projection': ccrs.Robinson()})\n",
    "ny.plot(transform=ccrs.PlateCarree(),ax=axs, cmap=plt.get_cmap('Blues'), vmin=0, vmax=0.5, cbar_kwargs={'label': '[$m^{3} m{-3}$]'})\n",
    "axs.plot(x,y,\"ro\",transform=ccrs.PlateCarree(),label=\"Newyork\")\n",
    "\n",
    "for p in [axs]:\n",
    "    # p.axes.add_feature(cartopy.feature.LAND, zorder=0, facecolor='gray')\n",
    "    p.axes.coastlines()\n",
    "\n",
    "axs.legend();\n",
    "axs.set_title(f\"Capurnicus Data from esa for {date}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcaa04-b399-4d63-8e6b-2a1c5ebffccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = regionmask.mask_geopandas(outline_buffered, \n",
    "                                 smap_lprm['soil_moisture'].lon.to_numpy(), \n",
    "                                 smap_lprm['soil_moisture'].lat.to_numpy(),\n",
    "                                 lon_name=\"lon\",\n",
    "                                 lat_name=\"lat\")\n",
    "\n",
    "cropped_ds_smap = smap_lprm.where(mask==0)\n",
    "cropped_ds_ccds = dssm.where(mask==0)\n",
    "\n",
    "plot_ds_smap = cropped_ds_smap[\"soil_moisture\"].isel(time=0)\n",
    "plot_ds_ccds = cropped_ds_ccds.isel(time=0)\n",
    "\n",
    "fig, [axs, axs2] = plt.subplots(1 ,2, figsize=(8,3), subplot_kw={'projection': ccrs.Robinson()})\n",
    "plot_ds_smap.plot(transform=ccrs.PlateCarree(),ax=axs, cmap=plt.get_cmap('Blues'), vmin=0, vmax=0.5, cbar_kwargs={'label': '[$m^{3} m{-3}$]'})\n",
    "time =  cropped_ds_smap.isel(time=0).time.values\n",
    "axs.set_title(f\"SMAP data for {str(time)[:10]}\")\n",
    "\n",
    "plot_ds_ccds.plot(transform=ccrs.PlateCarree(),ax=axs2, cmap=plt.get_cmap('Greens'), vmin=0, vmax=0.5, cbar_kwargs={'label': '[$m^{3} m{-3}$]'})\n",
    "time =  cropped_ds_ccds.isel(time=0).time.values\n",
    "axs2.set_title(f\"CCDS data for {str(time)[:10]}\")\n",
    "\n",
    "for p in [axs, axs2]:\n",
    "    p.axes.coastlines()\n",
    "    outline.plot(ax=p, edgecolor=\"C3\", facecolor=\"None\",transform=ccrs.PlateCarree())\n",
    "    p.set_xlim((p.get_xlim()[0]+300000,p.get_xlim()[1]))\n",
    "    p.set_ylim((p.get_ylim()[0]+350000,p.get_ylim()[1]+10000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5f78e-7a17-4f4e-8866-07927e771b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean_smap = cropped_ds_smap.mean(dim=[\"lat\",\"lon\"])\n",
    "series_smap_sm = ds_mean_smap[\"soil_moisture\"].to_pandas()\n",
    "series_smap_sm_pos = series_smap_sm[series_smap_sm> 0]\n",
    "\n",
    "ds_mean_ccds = cropped_ds_ccds.mean(dim=[\"lat\",\"lon\"])\n",
    "series_ccds_sm = ds_mean_ccds.to_pandas()\n",
    "series_ccds_sm_pos = series_ccds_sm[series_ccds_sm> 0]\n",
    "\n",
    "\n",
    "fig, [ax, ax2] = plt.subplots(1,2,figsize=(12,3))\n",
    "series_smap_sm_pos.plot(ax=ax)\n",
    "ax.set_title('SMAP data')\n",
    "\n",
    "\n",
    "series_ccds_sm_pos.plot(ax=ax2,label=\"CCDS\")\n",
    "stems_merged.loc[series_ccds_sm_pos.index.min():series_ccds_sm_pos.index.max()][\"LyeBrook (0.0508 m)\"].plot(ax=ax2, alpha=0.5,zorder=-1,label=\"In-situ\")\n",
    "ax2.legend()\n",
    "ax2.set_title('CCDS data compared to insitu observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc658f-f7f4-4a2f-b5fc-00a3aeb30bc0",
   "metadata": {},
   "source": [
    "The plots above show the different products. The ccds show some promising data when conmpared with the ground obersvation data. Obtaining SMAP data for the same period wasn't possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f1443-75a4-4aea-a36b-631692e8a42b",
   "metadata": {},
   "source": [
    "# 2.4 - Grace data & DEMs\n",
    "\n",
    "Dems have been used throughout, see GIS images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2222f-6a15-4fa1-9b0f-0ff65857a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{data_folder_grace}\\\\*.txt\")\n",
    "cnes_grgs = pd.read_csv(files[1], skiprows=45, skipinitialspace=True, delimiter=' ', index_col=['Time (years)'],\n",
    "                        names=['Time (seconds since 1970/01/01)', 'Time (days since 1950/01/01)','Time (years)', 'Equivalent Water Heights (cm)',\n",
    "                               'Linear model (cm)', 'Periodic model (cm)'])\n",
    "cost_g = pd.read_csv(files[2], skiprows=45, skipinitialspace=True, delimiter=' ', index_col=['Time (years)'],\n",
    "                        names=['Time (seconds since 1970/01/01)', 'Time (days since 1950/01/01)','Time (years)', 'Equivalent Water Heights (cm)',\n",
    "                               'Linear model (cm)', 'Periodic model (cm)'])\n",
    "jpl = pd.read_csv(files[3], skiprows=45, skipinitialspace=True, delimiter=' ', index_col=['Time (years)'],\n",
    "                        names=['Time (seconds since 1970/01/01)', 'Time_days_since','Time (years)', 'Equivalent Water Heights (cm)',\n",
    "                               'Linear model (cm)', 'Periodic model (cm)'])\n",
    "\n",
    "jpl['Time days'] = jpl.apply(lambda x: pd.Timestamp('1950-01-01') + datetime.timedelta(x.Time_days_since), axis=1)\n",
    "jpl_1 = jpl.copy()\n",
    "jpl_1.set_index(['Time days'], inplace=True)\n",
    "jpl_1.drop(inplace=True, columns=['Time (seconds since 1970/01/01)', 'Time_days_since', 'Linear model (cm)', 'Periodic model (cm)'])\n",
    "jpl_1['Storage [mm]'] = jpl_1['Equivalent Water Heights (cm)'] * 10\n",
    "jpl_1.drop(inplace=True, columns=['Equivalent Water Heights (cm)'])\n",
    "jpl_1.to_parquet(f'{data_folder_grace}\\\\jplStorage.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5cc01-ec15-45ee-9779-23e5bb108342",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "cnes_grgs['Equivalent Water Heights (cm)'].plot(marker='.', label='CNES/GRGS, RL05, DDK')\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=12, integer=True))\n",
    "ax.set_ylabel('Equivalent Water Heights [cm]')\n",
    "cost_g['Equivalent Water Heights (cm)'].plot(marker='s', label='COST-G, RL01, DDK5')\n",
    "jpl['Equivalent Water Heights (cm)'].plot(marker='^', label='JPL RL06, DDK5')\n",
    "ax.set_title('GRACE satellite gravity data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817d67a-3803-42ac-8ea7-5082c04e5490",
   "metadata": {},
   "source": [
    "#### GRACE (Gravity  Recovery And Climate Experiment)\n",
    "\n",
    "Grace data is obtained by a pair of satellites which follows each other on a polar orbit. The satellites are situated on a alitude of 500km and a distance of 220 km from each other. The plotted time series are created by three different groups. \n",
    "\n",
    "- CNES/GRGS: French National Space Center/ Research Group for Space Geodesy in Toulouse\n",
    "- COST-G: Combination Service for Time-variable Gravity Fields consortium\n",
    "- JPL: Jet Propulsion Laboratory, in Pasadena, California\n",
    "\n",
    "As can be seen, the COST-G model shows peaks with less magnitude compared to the other 2 models. \n",
    "\n",
    "Overall, the water height is changing within a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cb4c2-91f8-4d47-b209-6c9b70e896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "cnes_grgs['Linear model (cm)'].plot( label='CNES/GRGS, RL05, DDK')\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=12, integer=True))\n",
    "ax.set_ylabel('Linear model [m]')\n",
    "cost_g['Linear model (cm)'].plot(label='COST-G, RL01, DDK5')\n",
    "jpl['Linear model (cm)'].plot(label='JPL RL06, DDK5')\n",
    "ax.set_title('GRACE Linear Regression Model')\n",
    "ax.legend()\n",
    "ax.set_ylim((-4, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768850d4-0017-487d-acae-457a17e9feff",
   "metadata": {},
   "source": [
    "All three models show a decreasing trend in water height over past 20 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391e970-2321-4fdd-b4bc-fb77126c190e",
   "metadata": {},
   "source": [
    "# 2.5- Evaporation from Remote sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c6474-4621-429a-9cea-815154068c91",
   "metadata": {},
   "source": [
    "landsat was tried first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb897fc-e232-4c99-b05a-58d307bff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(data_folder + '\\\\EP\\\\landsat.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b51551-75e9-4af2-9a2f-8cfd84cdaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid showing warnings of cartopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pvar = ds.ET.isel(time=0)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())      # to use simple lat/lon \n",
    "pvar.plot.imshow(ax=ax, transform=ccrs.PlateCarree(),interpolation='nearest')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor='lightgray')\n",
    "ax.set_xlim((-75,-60))\n",
    "ax.set_ylim((40,50))\n",
    "outline.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a9ea3-d140-4bb4-b79b-8b1046cbf6c9",
   "metadata": {},
   "source": [
    "clearly falls outside our area, thus try another product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ee3c9-0dbb-43b4-b3f7-86c0d6bfa38e",
   "metadata": {},
   "source": [
    "### Use era5 data:\n",
    "code for automatic data retrieval is availible in notebook [2.5](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.5.EP_loading%20from%20remotesensing.ipynb) but was ommited for space reason\n",
    "\n",
    "These are large files so are not in the project folder, but processed data is availible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50d26c-b594-4a87-8f8b-f35e82139f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010']\n",
    "path = f\"C:\\\\Users\\\\david\\\\Documents\\\\EP_data_ENVM1502\\\\Era5_{year[0]}-{year[-1]}.nc\"\n",
    "ds = xr.open_dataset(path)\n",
    "ds.pev.values[ds.pev.values > 0] = 0\n",
    "cmap = mpl.cm.get_cmap(\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cb0ff-0495-490c-9373-bb1d22391482",
   "metadata": {},
   "source": [
    "We want to crop our region out of the mask as it currently includes a lot of ocean, when we take the mean, this causes discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfae997-23ed-40ad-849e-f952d1daac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvar = ds.pev.isel(time=4600)\n",
    "# ax = plt.axes(projection=ccrs.Orthographic(0, 0))  # to use the Orthographic projection \n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax =  fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "\n",
    "pvar.plot.imshow(ax=ax, transform=ccrs.PlateCarree(),interpolation='nearest',cmap=cmap)\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor='lightgray')\n",
    "ax.set_xlim((-77,-68))\n",
    "ax.set_ylim((38,47))\n",
    "outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "\n",
    "# cropping\n",
    "lon = np.arange(-76, -70+0.25, 0.25)\n",
    "lat = np.arange( 39,  46+0.25, 0.25)\n",
    "mask = regionmask.mask_geopandas(outline_buffered, lon, lat,lon_name=\"longitude\",lat_name=\"latitude\")\n",
    "cropped_ds = ds.where(mask==0)\n",
    "\n",
    "pvar = cropped_ds.pev.isel(time=4600)\n",
    "pvar.plot.imshow(ax=ax2, transform=ccrs.PlateCarree(),interpolation='nearest',cmap=cmap)\n",
    "ax2.coastlines()\n",
    "ax2.add_feature(cartopy.feature.OCEAN, facecolor='lightgray')\n",
    "ax2.set_xlim((-77,-68))\n",
    "ax2.set_ylim((38,47))\n",
    "outline.plot(ax=ax2, edgecolor=\"C3\", facecolor=\"None\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4c46e-9b02-4f00-a274-845e52305a57",
   "metadata": {},
   "source": [
    "Take mean & resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae454bd-ec41-4da7-adc6-aaeaa54f0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combine_mean = cropped_ds.mean(dim=[\"latitude\",\"longitude\"])\n",
    "ds_combine_daily = ds_combine_mean.resample(time=\"1D\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987558dc-8ec4-4590-a7b6-4fd87d6a3f19",
   "metadata": {},
   "source": [
    "Export to df to be used & repeat for all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e999c-5247-4c98-a73b-7b0cbbc48b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_pev = ds_combine_daily.pev.to_pandas()\n",
    "df_pev = pd.DataFrame(data=series_pev,columns=[\"Pev\"])\n",
    "df_pev.Pev = df_pev.Pev * - 1000 # m -> m & evap is evaporation is negative as it leaves the earth\n",
    "df_pev.index = df_pev.index - datetime.timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7406da-54bc-455e-8534-430416172799",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12,5))\n",
    "df_pev[\"Pev\"].plot(ax=ax)\n",
    "ax.set_title(\"Evaporation ERA5\")\n",
    "ax.set_ylabel(\"Evaporation in mm/day\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c71fc-f3a4-4095-ac45-e3f15353531a",
   "metadata": {},
   "source": [
    "Era5 seems very low which we dont fully understand, explored more in 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38c257-f8ab-4946-8d04-479a1065968f",
   "metadata": {},
   "source": [
    "# 2.6 - Data assimilation\n",
    "\n",
    "##### quite a bit of work has been done to prepare all the data:\n",
    "- [2.6.0](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.6.0.Data_assimilation-GoogleEE-IMERG-MODIS-GRACE%20data.ipynb) collects all the data using the google earth engine for IMERG, MODIS & GRACE\n",
    "- [2.6.1](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.6.1.Data_assimilation-GLEAM.ipynb) loads in monthly GLEAM evaporation data obtained from their sstp server between 1980-2020\n",
    "- [2.6.2](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.6.2.Data_assimilation-loading-all-GLEAM.ipynb) repeats this process for all the daily gleam data between 1980 - 2020, this data is large (17Gbs) thus only the processed data is included.\n",
    "\n",
    "\n",
    "Only the results are shown here (still reproducible), see notebook [2.6.7](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.6.7.Data_assimilation.ipynb) for more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfec7ec-3641-4488-8d5b-0fb24b1846ec",
   "metadata": {},
   "source": [
    "## load data\n",
    "limit to 2010-2015 due to storage constraints <br>\n",
    "**P** - ERA 5 obtained form ESMValTool - Forcings for the HBVmountain hydrological model: *with thanks to Rolf Hut*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917bdc1-4b64-4161-8933-11b6432353d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5 = xr.load_dataset(f\"{data_folder}\\\\ERA5_Group18_2010_2015.nc\")\n",
    "mask = regionmask.mask_geopandas(outline_buffered, \n",
    "                                 ds_era5['pr'].lon.to_numpy(), \n",
    "                                 ds_era5['pr'].lat.to_numpy(),\n",
    "                                 lon_name=\"lon\",\n",
    "                                 lat_name=\"lat\")\n",
    "cropped_ds_era5 = ds_era5.where(mask==0)\n",
    "\n",
    "# Pr in \"kg m-2 s-1\" -> *86400 mm/d\n",
    "pr_series = cropped_ds_era5[\"pr\"].mean(dim=[\"lat\",\"lon\"]).to_pandas() * 86400 \n",
    "df_era5_P = pd.DataFrame(pr_series,columns=[\"pr\"]).rename(columns={\"pr\":\"era5\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385efdc-281a-43ee-bdad-b941a9e27915",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMERG = pd.read_parquet(f\"{data_folder}\\\\P\\\\IMERG.parquet\").rename(columns={\"P\":\"IMERG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec41fb-2943-4564-9c1e-912a947b149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{data_folder}\\\\combined*.parquet\")\n",
    "df_all_data = pd.read_parquet(files[0]).loc[\"2010\":\"2015\"]\n",
    "station_data = df_all_data[[\"P\"]].rename(columns={\"P\":\"station\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddd220-aaee-45b8-bada-c422ab450ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "monlthy_p = [df_era5_P.resample(\"m\").sum(), IMERG.resample(\"m\").sum(), station_data.resample(\"m\").sum()]\n",
    "df_p = pd.concat(monlthy_p,axis = 1).loc[\"2010-03\":\"2014\"]\n",
    "\n",
    "# Compute monthly mean mP and variance vP of precipitation\n",
    "df_p[\"mp\"] = df_p.apply(lambda x: np.mean([x[f\"{col}\"] for col in x.index]),axis=1)\n",
    "df_p[\"vp\"] = df_p.apply(lambda x: (np.sum([(x[f\"{col}\"] - x.mp)**2 for col in x.index]))/(len(x.index)-1), axis=1)\n",
    "df_p[\"sp\"] = df_p.apply(lambda x: np.sqrt(x.vp) ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b24c76-9d3d-4e57-97a0-28ef71160696",
   "metadata": {},
   "source": [
    "**E** - Load in EP from data collected already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bf7d5-5232-4d0b-9617-035128135154",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_E = pd.read_parquet(f\"{data_folder}\\\\EP\\\\combined_PE.parquet\").rename(columns={\"Pev\":\"Era5\"}).resample('m').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee424b91-c3c3-4199-8d03-efddf91e325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIS = pd.read_parquet(f\"{data_folder}\\\\EP\\\\MODIS.parquet\").dropna()\n",
    "MODIS[\"PET\"] = MODIS[\"PET\"] * 0.1 # \"kg/m^2/8day\"  -> mm: scale factor 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49256e6a-ceb5-4c41-994e-20afeafc32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIS_monthly = MODIS.resample('m').sum()\n",
    "MODIS_monthly.rename(columns={\"PET\":\"MODIS\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b15eb5-7b92-4a90-a79b-ce5becf86ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLEAM = pd.read_parquet(f\"{data_folder}\\\\EP\\\\GLEAM.parquet\").loc[\"2010\":\"2015\"].rename(columns={\"Pe\":\"GLEAM\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f58b94-6b92-481c-92cd-908f70f4cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.concat([ERA5_E,MODIS_monthly, GLEAM],axis=1).loc[\"2010-03\":\"2014\"]\n",
    "\n",
    "# Compute monthly mean mE and variance vE of evaporation\n",
    "df_e[\"me\"] = df_e.apply(lambda x: np.mean([x[f\"{col}\"] for col in x.index]),axis=1)\n",
    "df_e[\"ve\"] = df_e.apply(lambda x: (np.sum([(x[f\"{col}\"] - x.me)**2 for col in x.index]))/(len(x.index)-1), axis=1)\n",
    "df_e[\"se\"] = df_e.apply(lambda x: np.sqrt(x.ve),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88216fc9-6c26-4038-9f5e-9a4939c3a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax2, ax] = plt.subplots(1,2, figsize=(15,3))\n",
    "fig.tight_layout(w_pad=5)\n",
    "\n",
    "df_e[[\"Era5\",\"MODIS\",\"GLEAM\"]].plot(ax=ax)\n",
    "df_e['me'].plot(ax=ax,lw=0,color=\"C3\",marker=\".\",label=\"Mean E\")\n",
    "ax.fill_between(df_e.index,(df_e[\"me\"]+df_e[\"se\"]).values,(df_e[\"me\"]-df_e[\"se\"]).values,color=\"grey\",alpha=0.7)\n",
    "ax.set_title(\"Monthly evaporation data\") \n",
    "ax.set_ylabel(\"E (mm/month)\")\n",
    "ax.set_xlabel(\"Month number\");\n",
    "ax.legend(bbox_to_anchor=(0.464,0.588))\n",
    "\n",
    "df_p[[\"era5\",\"IMERG\",\"station\"]].plot(ax=ax2)\n",
    "df_p['mp'].plot(ax=ax2,lw=0,color=\"C3\",marker=\".\",label=\"Mean prcp\")\n",
    "ax2.fill_between(df_p.index,(df_p[\"mp\"]+df_p[\"sp\"]).values,(df_p[\"mp\"]-df_p[\"sp\"]).values,color=\"grey\",alpha=0.7)\n",
    "ax2.set_title(\"Monthly precipitation data\") \n",
    "ax2.set_ylabel(\"P (mm/month)\")\n",
    "ax2.set_xlabel(\"Month number\")\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42e517-f207-40a1-9f33-60b2e8cdbc23",
   "metadata": {},
   "source": [
    "Here we see the issue with the era 5 evaporation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829b21f-73ae-48dd-9134-70d8b7e16a5f",
   "metadata": {},
   "source": [
    "**Q** - station data already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de02df-78b1-470e-bae8-c076b22eddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = df_all_data[[\"Q\"]].resample(\"m\").sum().loc[\"2010-03\":\"2014\"]\n",
    "# Compute monthly mean mQ and variance vQ of river discharge\n",
    "df_q[\"mq\"] = df_q.apply(lambda x: np.mean([x[f\"{col}\"] for col in x.index]),axis=1)\n",
    "df_q[\"vq\"] = df_q.apply(lambda x: 0.1 * x.mq, axis=1)\n",
    "df_q[\"sq\"] = df_q.apply(lambda x: np.sqrt(x.vq),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7de5b-da49-4e3f-a571-64b3fca287b6",
   "metadata": {},
   "source": [
    "**S** - from Earth engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09c6ae-0251-4f10-b731-d826cb154ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GRACE = pd.read_parquet(f\"{data_folder}\\\\Grace\\\\GRACE.parquet\").loc[\"2010-03\":\"2014\"]\n",
    "df_GRACE_error = pd.read_parquet(f\"{data_folder}\\\\GRace\\\\GRACE_error.parquet\").loc[\"2010-03\":\"2014-12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d08ba-66cc-40b1-8b6b-2f7b12836624",
   "metadata": {},
   "source": [
    "Bit of data management & magic to get it working with the others. Due to missing data there is some uncertainty in this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53356333-1c0d-4b17-9faf-f3bc9957d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_in = pd.concat([df_GRACE,df_GRACE_error],axis=1).resample(\"m\").mean()\n",
    "wanted_index = df_q.apply(lambda x: f'{x.name.year}-{x.name.month}', axis=1).copy()\n",
    "curent_index = df_s_in.apply(lambda x: f'{x.name.year}-{x.name.month}', axis=1)\n",
    "df_index = pd.DataFrame(wanted_index).reset_index().set_index(0).rename(columns={\"index\":\"fulldate\"})\n",
    "df_missing = df_index.loc[list(set(wanted_index.values).difference(set(curent_index.values)))].sort_values(\"fulldate\").set_index(\"fulldate\")\n",
    "df_missing['graceJPL'] = 0\n",
    "df_missing['graceJPLerror'] = 10**9\n",
    "df_s = pd.concat([df_s_in,df_missing[['graceJPL','graceJPLerror']]])\n",
    "df_s[\"graceJPL\"]  = df_s.apply(lambda x: 0 if np.isnan(x.graceJPL) else x.graceJPL, axis=1)\n",
    "df_s[\"graceJPLerror\"]  = df_s.apply(lambda x: 10**9 if np.isnan(x.graceJPLerror) else x.graceJPLerror, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8a94e-618e-428c-9267-900689876118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute monthly mean mS and variance vS of water storage\n",
    "df_s[\"ms\"]  = df_s.apply(lambda x: x.graceJPL if not np.isnan(x.graceJPL) else 0, axis=1) # 0  \n",
    "df_s[\"ss\"]  = df_s.apply(lambda x: x.graceJPLerror if x.graceJPLerror > -9999 else np.nan, axis=1) #10**9 if np.isnan(x.graceJPLerror) else \n",
    "df_s[\"vs\"]  = df_s.apply(lambda x: x.graceJPLerror**2 if x.graceJPLerror > -9999 else 10**9, axis=1) #10**9 if np.isnan(x.graceJPLerror) else "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6314a4-c370-49dc-aa6d-e204cdbeca13",
   "metadata": {},
   "source": [
    "**then run the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d32876-98a3-4252-ac09-01fb45467bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_product(m1, v1, m2, v2):\n",
    "    w1 = v2 / (v1 + v2)\n",
    "    w2 = v1 / (v1 + v2)\n",
    "    m = w1 * m1 + w2 * m2\n",
    "    v = w1 * v1 \n",
    "    # print(v, w2*v2)\n",
    "    if not np.isclose(v, w2 * v2):\n",
    "        print(f\"Issue with V2, should be same {v}, {w2 * v2}\")\n",
    "        raise AssertionError\n",
    "    return m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315c430-9017-48e1-b396-6f8bced7686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_water_balance(mP, vP, mE, vE, mQ, vQ, mS, vS):\n",
    "    # Specify prior mean and variance for initial storage S0 (first month)\n",
    "    mS0 = 0\n",
    "    vS0 = 200 * 200#large variance to indicate large uncertainty\n",
    "    # print([len(mP), len(vP),len(mE), len(vE), len(mQ), len(vQ), len(mS), len(vS)])\n",
    "    # Initialize posteriors\n",
    "    # We compute these in forward loop (filtering posteriors) and then update them in backward loop (smoothing posteriors)\n",
    "    nt = len(mP)#number of months in time-series\n",
    "    mPostP = np.zeros(nt)#posterior mean of precipitation in each month\n",
    "    vPostP = np.zeros(nt)#posterior variance of precipitation in each month\n",
    "    mPostE = np.zeros(nt)#etc...\n",
    "    vPostE = np.zeros(nt)\n",
    "    mPostQ = np.zeros(nt)\n",
    "    vPostQ = np.zeros(nt)\n",
    "    mPostS = np.zeros(nt)\n",
    "    vPostS = np.zeros(nt)\n",
    "    \n",
    "    # Forward loop (filtering)\n",
    "    for t in range(nt):\n",
    "        if t != 0: # Step 1: predict - compute forward distribution to S\n",
    "            mS0, vS0 = mPostS[t-1], vPostS[t-1]\n",
    "        mS_1  =  mP[t] + mS0 - mE[t] - mQ[t]\n",
    "        vS_1  =  vP[t] + vS0 + vE[t] + vQ[t]\n",
    "        mPostS[t], vPostS[t] = gaussian_product(mS_1, vS_1, mS[t], vS[t]) # Step 2: update - compute filtering posterior of S        \n",
    "    \n",
    "    # Backward loop (smoothing)\n",
    "    mToS, vToS = 0.0, 1e+9#represents backward distribution to S - initialize to wide Gaussian for last month\n",
    "    for t in reversed(range(nt)):\n",
    "        mPostS[t], vPostS[t] = gaussian_product(mPostS[t], vPostS[t], mToS, vToS) # Step 1: smoothing posterior of S\n",
    "        \n",
    "        mBackS, vBackS = gaussian_product(mToS, vToS, mS[t], vS[t])               # Step 2: posteriors of P, E, and Q  ### backwards S\n",
    "        if t != 0:\n",
    "            mS0, vS0 = mPostS[t-1], vPostS[t-1]\n",
    "        else: \n",
    "            mS0, vS0 = 0, 200*200\n",
    "        # Q = P - S + S0 - E\n",
    "        mQ_1  =  mP[t] - mBackS + mS0 - mE[t]\n",
    "        vQ_1  =  vP[t] + vBackS + vS0 + vE[t]\n",
    "        mPostQ[t], vPostQ[t] = gaussian_product(mQ[t],vQ[t], mQ_1, vQ_1)\n",
    "        # P  = S - S0 + E + Q\n",
    "        mP_1  =  + mBackS - mS0 + mE[t] + mQ[t]\n",
    "        vP_1  =  + vBackS + vS0 + vE[t] + vQ[t]\n",
    "        mPostP[t], vPostP[t] = gaussian_product(mP[t],vP[t], mP_1, vP_1)\n",
    "        # E  = P + S0 - Q -S \n",
    "        mE_1  =  mP[t] - mBackS + mS0  - mQ[t]\n",
    "        vE_1  =  vP[t] + vBackS + vS0  + vQ[t]\n",
    "        mPostE[t], vPostE[t] = gaussian_product(mE[t],vE[t], mE_1, vE_1)\n",
    "        \n",
    "        mToS = mBackS - mP[t] + mE[t] + mQ[t]                                     # Step 3: backward distribution to S0\n",
    "        vToS = vBackS + vP[t] + vE[t] + vQ[t]\n",
    "    \n",
    "    mPostS0, vPostS0 = gaussian_product(mToS, vToS, mS0, vS0) # After backward loop: compute posterior mean and variance of S0 (initial storage first month)\n",
    "    return mPostP, vPostP, mPostE, vPostE, mPostQ, vPostQ, mPostS, vPostS, mPostS0, vPostS0 # Return final water balance estimates (posterior means and variances of monthly estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921a861-1901-4cb5-8710-23a595432450",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = close_water_balance(df_p[\"mp\"].values, df_p[\"vp\"].values, df_e[\"me\"].values, df_e[\"ve\"].values, df_q[\"mq\"].values, df_q[\"vq\"].values, df_s[\"ms\"].values, df_s[\"vs\"].values)\n",
    "mPostP, vPostP, mPostE, vPostE, mPostQ, vPostQ, mPostS, vPostS, mPostS0, vPostS0 = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59a177-03de-45f4-bdcc-edf1f6182068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p[\"mPostP\"],df_p[\"vPostP\"],df_e[\"mPostE\"],df_e[\"vPostE\"] = mPostP,vPostP,mPostE,vPostE\n",
    "df_q[\"mPostQ\"],df_q[\"vPostQ\"],df_s[\"mPostS\"],df_s[\"vPostS\"] = mPostQ,vPostQ,mPostS,vPostS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07bc66-cda1-4672-ac0b-3efce1fa5cfa",
   "metadata": {},
   "source": [
    "Due to small errors in the numerical methods the water balance has some erros but they are close enough to zero to call it a sucess (left plot). \n",
    "\n",
    "Right shows that the gleam data is the best overal when closing the monthly waterbalance. This has already be done above & this is why the basin falls so nicely on the budyko curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727b393-d317-4c9b-b8a2-75ac15389672",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_balance = np.zeros(len(mPostP))\n",
    "storage_final = np.zeros(len(mPostP))\n",
    "\n",
    "fig, [ax_l, ax] = plt.subplots(1,2,figsize=(15,5))\n",
    "for t in range(len(mPostP)):\n",
    "    if t == 0:\n",
    "        S0 = mPostS0\n",
    "    else:\n",
    "        S0 = mPostS[t-1]\n",
    "    water_balance[t] = mPostS[t] - mPostP[t]  + mPostE[t] + mPostQ[t] - S0\n",
    "    storage_final[t] = mPostP[t]  - mPostE[t] - mPostQ[t] + S0\n",
    "ax_l.plot(water_balance,\"ro\")\n",
    "ax_l.axhline(0)\n",
    "ax_l.set_title(\"Error in the water balance\")\n",
    "\n",
    "df_e[[\"Era5\",\"MODIS\",\"GLEAM\"]].plot(ax=ax)\n",
    "df_e['mPostE'].plot(ax=ax,lw=0,color=\"C3\",marker=\".\",label=\"post mean\")\n",
    "ax.fill_between(df_e.index,(df_e[\"me\"]+df_e[\"se\"]).values,(df_e[\"me\"]-df_e[\"se\"]).values,color=\"grey\",alpha=0.3, label=\"initial error bounds\")\n",
    "ax.fill_between(df_e.index,(df_e[\"mPostE\"]+np.sqrt(df_e[\"vPostE\"]).values),(df_e[\"mPostE\"]-np.sqrt(df_e[\"vPostE\"]).values),color=\"grey\",alpha=0.9,label=\"post error bounds\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "ax.set_title(\"Mean monthly evaporation data with bounds\") \n",
    "ax.set_ylabel(\"E (mm/month)\")\n",
    "ax.set_xlabel(\"Month number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629390e4-c580-48a0-9ec7-33ea787680ef",
   "metadata": {},
   "source": [
    "# 2.8 - Climate data\n",
    "\n",
    "notebook [2.8](https://github.com/Daafip/ENVM1502-Catchment-model/blob/main/Python%20files/2.8.Climate_predictions.ipynb) provides an automated way to downlaod data, we will simply load it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6176b53-9b47-4a6c-85ca-07c559bed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code shows one map\n",
    "ifile = glob.glob(f'{data_folder}\\\\Climate Predictions\\\\*.nc')\n",
    "ds_534 = xr.open_mfdataset(ifile[0]) # open dataset as x-array\n",
    "ds_585 = xr.open_mfdataset(ifile[1]) # open dataset as x-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c450a4-f373-46a4-90a9-9898cf0ac3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_series_out = []\n",
    "names = [\"EC-Earth3_ssp534\",\"IPSL-CM6A-LR_ssp585\"]\n",
    "fig, [ax, ax2, ax3] = plt.subplots(1,3,figsize=(17,5))\n",
    "for index, ds in enumerate([ds_534,ds_585]):\n",
    "    mean_lat = ds.mean(dim=\"lat\")\n",
    "    mean_lat_lon = mean_lat.mean(dim=\"lon\")\n",
    "    series_out = mean_lat_lon.pr.to_pandas() * 86400\n",
    "    series_out.plot(ax=ax, marker=\".\",markersize=3,lw=0.1,label=names[index],zorder=1-index)\n",
    "    ax.set_title(\"daily precipitation values\")\n",
    "    ax.set_ylabel(\"mm/day\")\n",
    "    ax.legend()\n",
    "    lst_series_out.append(series_out)\n",
    "\n",
    "for index, series_out in enumerate(lst_series_out):    \n",
    "    df = pd.DataFrame(data=series_out)\n",
    "    ax2.axhline(df.resample(\"m\").sum()[0].mean(),color=f\"C{index}\",label=names[index], alpha=0.8,lw=1.5,zorder=5)\n",
    "    ax2.axhline(df.resample(\"m\").sum()[0].mean(),color=\"w\",lw=2,zorder=4)\n",
    "    df = df.resample(\"m\").sum()\n",
    "    ax2.plot(df[0].values,marker=\".\",zorder=1-index,color=f\"C{index}\")\n",
    "    ax2.set_ylabel(\"mm/month\")\n",
    "    ax2.set_title(\"monthly sum precipitation for two projections\")\n",
    "ax2.axhline(combined_forcing[[\"P\"]].resample(\"m\").sum().mean().values,color=\"C3\",lw=1.5,label=\"1980-2020 mean\",zorder=5)\n",
    "ax2.axhline(combined_forcing[[\"P\"]].resample(\"m\").sum().mean().values,color=\"w\",lw=2,zorder=4)\n",
    "ax2.legend()\n",
    "\n",
    "for index, series_out in enumerate(lst_series_out):    \n",
    "    df = pd.DataFrame(data=series_out)\n",
    "    df.resample(\"y\").sum().plot(ax=ax3,marker=\".\",label=names[index],zorder=1-index)\n",
    "    ax3.set_ylabel(\"mm/month\")\n",
    "    ax3.set_title(\"yearly sum precipitation for two projections\")\n",
    "ax3.legend(names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094632a9-5300-4c22-ad1b-eeecff7ae9f9",
   "metadata": {},
   "source": [
    "SSP585 shows a higher level of monthly rainfall than SSP534, showing increasing flood peaks could be an issue to look out for. At the same time the lower peaks are also more extreme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b3467-8687-4d0e-b23e-74b338218785",
   "metadata": {},
   "source": [
    "# 3 modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88415c9-64b8-4825-a179-6b32bf13e704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
